{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data reduction for the hap-hd channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.15'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "import astropy.units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.convolution import Tophat2DKernel\n",
    "from regions import CircleSkyRegion, RectangleSkyRegion\n",
    "\n",
    "from gammapy.detect import compute_lima_on_off_image\n",
    "from gammapy.data import DataStore\n",
    "from gammapy.irf import make_mean_psf\n",
    "from gammapy.maps import Map, MapAxis, WcsGeom\n",
    "from gammapy.cube import (\n",
    "    MapDatasetMaker,\n",
    "    PSFKernel,\n",
    "    MapDataset,\n",
    "    RingBackgroundMaker,\n",
    "    SafeMaskMaker,\n",
    "    #RingBackgroundEstimator,\n",
    ")\n",
    "from gammapy.modeling.models import (\n",
    "    SkyModel,\n",
    "    BackgroundModel,\n",
    "    PowerLawSpectralModel,\n",
    "    PowerLaw2SpectralModel,\n",
    "    PointSpatialModel,\n",
    "    ExpCutoffPowerLawSpectralModel,\n",
    "    SkyDiffuseCube,\n",
    "    TemplateSpatialModel\n",
    ")\n",
    "from gammapy.modeling import Fit\n",
    "from astropy.time import Time\n",
    "\n",
    "src_pos = SkyCoord(359.94, -0.04, unit=\"deg\", frame=\"galactic\")\n",
    "\n",
    "import gammapy\n",
    "gammapy.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing FoVBackground class (for pre 0.16 gammapy versions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "##FoV background estimation\n",
    "import logging\n",
    "from gammapy.maps import Map\n",
    "from gammapy.modeling import Fit, Datasets\n",
    "\n",
    "\n",
    "class FoVBackgroundMaker:\n",
    "    \"\"\"Normalize template background on the whole field-of-view.\n",
    "\n",
    "    The dataset background model can be simply scaled (method=\"scale\") or fitted (method=\"fit\")\n",
    "    on the dataset counts.\n",
    "\n",
    "    The normalization is performed outside the exclusion mask that is passed on init.\n",
    "\n",
    "    If a SkyModel is set on the input dataset and method is 'fit', its are frozen during\n",
    "    the fov normalization fit.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    method : str in ['fit', 'scale']\n",
    "        the normalization method to be applied. Default 'scale'.\n",
    "    exclusion_mask : `~gammapy.maps.WcsNDMap`\n",
    "        Exclusion mask\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, method=\"scale\", exclusion_mask=None):\n",
    "        if method in [\"fit\", \"scale\"]:\n",
    "            self.method = method\n",
    "        else:\n",
    "            raise ValueError(f\"Incorrect method for FoVBackgroundMaker: {method}.\")\n",
    "        self.exclusion_mask = exclusion_mask\n",
    "\n",
    "\n",
    "    def run(self, dataset):\n",
    "        \"\"\"Run FoV background maker.\n",
    "\n",
    "        Fit the background model norm\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : `~gammapy.cube.fit.MapDataset`\n",
    "            Input map dataset.\n",
    "\n",
    "        \"\"\"\n",
    "        mask_fit = dataset.mask_fit\n",
    "        dataset.mask_fit = self._reproject_exclusion_mask(dataset)\n",
    "\n",
    "        if self.method is \"fit\":\n",
    "            self._fit_bkg(dataset)\n",
    "        else:\n",
    "            self._scale_bkg(dataset)\n",
    "\n",
    "        dataset.mask_fit = mask_fit\n",
    "        return dataset\n",
    "\n",
    "\n",
    "    def _reproject_exclusion_mask(self, dataset):\n",
    "        \"\"\"Reproject the exclusion on the dataset geometry\"\"\"\n",
    "        mask_map = Map.from_geom(dataset.counts.geom)\n",
    "        if self.exclusion_mask is not None:\n",
    "            coords = dataset.counts.geom.get_coord()\n",
    "            vals = self.exclusion_mask.get_by_coord(coords)\n",
    "            mask_map.data += vals\n",
    "\n",
    "        return mask_map.data.astype(\"bool\")\n",
    "\n",
    "    def _fit_bkg(self, dataset):\n",
    "        \"\"\"Fit the FoV background model on the dataset counts data\"\"\"\n",
    "\n",
    "        # freeze all model components not related to background model\n",
    "        datasets = Datasets([dataset])\n",
    "\n",
    "        parameters_frozen = []\n",
    "        for par in datasets.parameters:\n",
    "            parameters_frozen.append(par.frozen)\n",
    "            if par not in dataset.background_model.parameters:\n",
    "                par.frozen = True\n",
    "\n",
    "        #!!!AL: relax titlt : BE CARREFULL !!!\n",
    "        dataset.background_model.tilt.frozen=False\n",
    "        \n",
    "        fit = Fit(datasets)\n",
    "        fit_result = fit.run()\n",
    "        if fit_result.success is False:\n",
    "            print(\"FoVBackgroundMaker failed. No fit convergence\")\n",
    "            \n",
    "\n",
    "        # Unfreeze parameters\n",
    "        for i, par in enumerate(datasets.parameters):\n",
    "            par.frozen = parameters_frozen[i]\n",
    "\n",
    "    def _scale_bkg(self, dataset):\n",
    "        \"\"\"Fit the FoV background model on the dataset counts data\"\"\"\n",
    "        mask = dataset.mask\n",
    "        count_tot = dataset.counts.data[mask].sum()\n",
    "        bkg_tot = dataset.background_model.map.data[mask].sum()\n",
    "\n",
    "        if count_tot <= 0.0:\n",
    "            print(\"FoVBackgroundMaker failed. No counts found outside exclusion mask\")\n",
    "        elif bkg_tot <= 0.0:\n",
    "            print(\"FoVBackgroundMaker failed. No positive background found outside exclusion mask\")\n",
    "        else:\n",
    "            scale = count_tot / bkg_tot\n",
    "            dataset.background_model.norm.value = scale\n",
    "            #print(\"bkg scale = \",scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datastore creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_store_hess1 = DataStore.from_dir(\"$GAMMAPY_DATA/hap-hd_Prod05/hess1/std_ImPACT_fullEnclosure\")\n",
    "data_store_hess1u = DataStore.from_dir(\"$GAMMAPY_DATA/hap-hd_Prod05/hess1u/std_ImPACT_fullEnclosure\")\n",
    "data_store_hess2 = DataStore.from_dir(\"$GAMMAPY_DATA/hap-hd_Prod05/hess2/std_ImPACT_fullEnclosure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sky region selection : observation pointing within this region will be selected \n",
    "#(this can be redundant with maximum offset selection, but using the max_offset should be prefered)\n",
    "\n",
    "from astropy.coordinates import Angle\n",
    "\n",
    "selection = dict(type='sky_circle', frame='galactic',\n",
    "                 lon=Angle(0, 'deg'),\n",
    "                 lat=Angle(0, 'deg'),\n",
    "                 radius=Angle(2, 'deg'),\n",
    "                 border=Angle(0, 'deg'))\n",
    "\n",
    "\n",
    "# selecting the admitted range of zenithal angle, such selection can be done for any column of the obs_table\n",
    "selectionZEN = dict(type='par_box', variable='ZEN_PNT', value_range=[0., 90])\n",
    "\n",
    "obs_table1 = data_store_hess1.obs_table.select_observations(selection)\n",
    "obs_table1 = obs_table1.select_observations(selectionZEN)\n",
    "\n",
    "obs_table1u = data_store_hess1u.obs_table.select_observations(selection)\n",
    "obs_table1u = obs_table1u.select_observations(selectionZEN)\n",
    "\n",
    "obs_table2 = data_store_hess2.obs_table.select_observations(selection)\n",
    "obs_table2 = obs_table2.select_observations(selectionZEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed observations : 20191\n",
      "Removed observations : 27162\n",
      "Removed observations : 27204\n",
      "Removed observations : 27329\n",
      "Removed observations : 27578\n",
      "Removed observations : 27607\n",
      "Removed observations : 27638\n",
      "Removed observations : 146924\n",
      "Removed observations : 146965\n",
      "Removed observations : 147004\n",
      "Removed observations : 153153\n"
     ]
    }
   ],
   "source": [
    "# Removing observations lacking IRFs (effective area, energy dispersion, point spread function and background model)\n",
    "\n",
    "\n",
    "#HESS1\n",
    "ids = obs_table1[\"OBS_ID\"].tolist()\n",
    "observations1 = data_store_hess1.get_observations(ids, skip_missing=True)\n",
    "\n",
    "for obs in observations1:\n",
    "    try:\n",
    "        obs.aeff\n",
    "        obs.edisp\n",
    "        obs.psf\n",
    "        obs.bkg\n",
    "            \n",
    "    except:\n",
    "        ids.remove(obs.obs_id)\n",
    "        print(\"Removed observations : \" + str(obs.obs_id))\n",
    "        \n",
    "observations1 = data_store_hess1.get_observations(ids, skip_missing=True)\n",
    "obs_table1 = obs_table1.select_obs_id(ids)\n",
    "\n",
    "\n",
    "#HESS1U\n",
    "ids = obs_table1u[\"OBS_ID\"].tolist()\n",
    "observations1u = data_store_hess1u.get_observations(ids, skip_missing=True)\n",
    "\n",
    "for obs in observations1u:\n",
    "    try:\n",
    "        obs.aeff\n",
    "        obs.edisp\n",
    "        obs.psf\n",
    "        obs.bkg\n",
    "            \n",
    "    except:\n",
    "        ids.remove(obs.obs_id)\n",
    "        print(\"Removed observations : \" + str(obs.obs_id))\n",
    "        \n",
    "observations1u = data_store_hess1u.get_observations(ids, skip_missing=True)\n",
    "obs_table1u = obs_table1u.select_obs_id(ids)\n",
    "\n",
    "\n",
    "# HESS2\n",
    "ids = obs_table2[\"OBS_ID\"].tolist()\n",
    "observations2 = data_store_hess2.get_observations(ids, skip_missing=True)\n",
    "\n",
    "for obs in observations2:\n",
    "    try:\n",
    "        obs.aeff\n",
    "        obs.edisp\n",
    "        obs.psf\n",
    "        obs.bkg\n",
    "            \n",
    "    except:\n",
    "        ids.remove(obs.obs_id)\n",
    "        print(\"Removed observations : \" + str(obs.obs_id))\n",
    "        \n",
    "observations2 = data_store_hess2.get_observations(ids, skip_missing=True)\n",
    "obs_table2 = obs_table2.select_obs_id(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping valid observations by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time intervals used for selection\n",
    "\n",
    "t2004  = dict(type='time_box', time_range= Time(['2004-01-01T00:00:00', '2004-12-31T23:59:59']))\n",
    "t2005  = dict(type='time_box', time_range= Time(['2005-01-01T00:00:00', '2005-12-31T23:59:59']))\n",
    "t2006  = dict(type='time_box', time_range= Time(['2006-01-01T00:00:00', '2006-12-31T23:59:59']))\n",
    "t2007  = dict(type='time_box', time_range= Time(['2007-01-01T00:00:00', '2007-12-31T23:59:59']))\n",
    "t2008  = dict(type='time_box', time_range= Time(['2008-01-01T00:00:00', '2008-12-31T23:59:59']))\n",
    "t2009  = dict(type='time_box', time_range= Time(['2009-01-01T00:00:00', '2009-12-31T23:59:59']))\n",
    "t2010  = dict(type='time_box', time_range= Time(['2010-01-01T00:00:00', '2010-12-31T23:59:59']))\n",
    "t2011  = dict(type='time_box', time_range= Time(['2011-01-01T00:00:00', '2011-12-31T23:59:59']))\n",
    "t2012  = dict(type='time_box', time_range= Time(['2012-01-01T00:00:00', '2012-12-31T23:59:59']))\n",
    "t2013  = dict(type='time_box', time_range= Time(['2013-01-01T00:00:00', '2013-12-31T23:59:59']))\n",
    "t2014  = dict(type='time_box', time_range= Time(['2014-01-01T00:00:00', '2014-12-31T23:59:59']))\n",
    "t2015  = dict(type='time_box', time_range= Time(['2015-01-01T00:00:00', '2015-12-31T23:59:59']))\n",
    "t2016  = dict(type='time_box', time_range= Time(['2016-01-01T00:00:00', '2016-12-31T23:59:59']))\n",
    "t2017  = dict(type='time_box', time_range= Time(['2017-01-01T00:00:00', '2017-12-31T23:59:59']))\n",
    "t2018  = dict(type='time_box', time_range= Time(['2018-01-01T00:00:00', '2018-12-31T23:59:59']))\n",
    "t2019  = dict(type='time_box', time_range= Time(['2019-01-01T00:00:00', '2019-12-31T23:59:59']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_intervals = { 2004 : t2004, 2005 : t2005, 2006 : t2006, 2007 : t2007,\n",
    "                      2008 : t2008, 2009 : t2009, 2010 : t2010, 2011 : t2011,\n",
    "                      2012 : t2012, 2013 : t2013, 2014 : t2014, 2015 : t2015,\n",
    "                      2016 : t2016, 2017 : t2017, 2018 : t2018, 2019 : t2019}\n",
    "\n",
    "# HESS1\n",
    "yearly_obs1 = dict()\n",
    "\n",
    "for year in range(2004,2020) :\n",
    "    \n",
    "    obs_table_year = obs_table1.select_observations(year_intervals[year])\n",
    "    ids = obs_table_year[\"OBS_ID\"].tolist()\n",
    "    observations_year = data_store_hess1.get_observations(ids, skip_missing=True)\n",
    "    yearly_obs1[year] = observations_year\n",
    "\n",
    "#HESS1U    \n",
    "yearly_obs1u = dict()\n",
    "\n",
    "for year in range(2004,2020) :\n",
    "    \n",
    "    obs_table_year = obs_table1u.select_observations(year_intervals[year])\n",
    "    ids = obs_table_year[\"OBS_ID\"].tolist()\n",
    "    observations_year = data_store_hess1u.get_observations(ids, skip_missing=True)\n",
    "    yearly_obs1u[year] = observations_year\n",
    "\n",
    "#HESS2    \n",
    "yearly_obs2 = dict()\n",
    "\n",
    "for year in range(2004,2020) :\n",
    "    \n",
    "    obs_table_year = obs_table2.select_observations(year_intervals[year])\n",
    "    ids = obs_table_year[\"OBS_ID\"].tolist()\n",
    "    observations_year = data_store_hess2.get_observations(ids, skip_missing=True)\n",
    "    yearly_obs2[year] = observations_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the geometry for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "emin, emax = [0.5, 100] * u.TeV\n",
    "\n",
    "energy_axis = MapAxis.from_bounds(\n",
    "    emin.value, emax.value, 20, unit=\"TeV\", name=\"energy\", interp=\"log\"\n",
    ")\n",
    "geom = WcsGeom.create(\n",
    "    skydir=(0, 0),\n",
    "    binsz=0.02,\n",
    "    width=(10, 8),\n",
    "    coordsys=\"GAL\",\n",
    "    proj=\"CAR\",\n",
    "    axes=[energy_axis],\n",
    ")\n",
    "\n",
    "energy_axis_true = MapAxis.from_bounds(\n",
    "    0.3, 200, 30, unit=\"TeV\", name=\"energy\", interp=\"log\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the MapDataset objects\n",
    "\n",
    "mapdataset_tot = MapDataset.create(geom=geom, energy_axis_true=energy_axis_true, name=\"maptot\")\n",
    "\n",
    "mapdataset_dict = {}\n",
    "\n",
    "for k in range (2004,2020):\n",
    "    \n",
    "    name = \"map\" + str(k)\n",
    "    mapdataset_dict[k] = MapDataset.create(\n",
    "    geom=geom, energy_axis_true=energy_axis_true, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset containing all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25min, sys: 3.18 s, total: 25min 3s\n",
      "Wall time: 25min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "exclusion_region = RectangleSkyRegion(src_pos, 3*u.deg, 1*u.deg)\n",
    "exclusion_mask = geom.region_mask([exclusion_region], inside=False)\n",
    "exclusion_mask = Map.from_geom(geom, data=exclusion_mask)\n",
    "\n",
    "offset_max = 1.8 * u.deg #on peut le réduire pour ne pas prendre les obs trop éloignées du centre de 2.0 deg à 1.8 deg\n",
    "maker = MapDatasetMaker()\n",
    "maker_safe_mask = SafeMaskMaker(methods=[\"offset-max\", \"bkg-peak\"], offset_max=offset_max)\n",
    "maker_bkg = FoVBackgroundMaker(\"scale\", exclusion_mask)\n",
    "\n",
    "spectrum = PowerLaw2SpectralModel(index=2.3)\n",
    "\n",
    "\n",
    "for obs in observations1:\n",
    "        # First a cutout of the target map is produced\n",
    "    cutout = mapdataset_dict[year].cutout(obs.pointing_radec, width=2 * offset_max)\n",
    "\n",
    "        # A MapDataset is filled in this cutout geometry\n",
    "    dataset = maker.run(cutout, obs)\n",
    "        \n",
    "        # The data quality cut is applied\n",
    "    dataset = maker_safe_mask.run(dataset, obs)\n",
    "    \n",
    "    dataset = maker_bkg.run(dataset)\n",
    "        \n",
    "        # The resulting dataset cutout is stacked onto the final one\n",
    "    mapdataset_tot.stack(dataset)\n",
    "\n",
    "    \n",
    "for obs in observations1u:\n",
    "        # First a cutout of the target map is produced\n",
    "    cutout = mapdataset_dict[year].cutout(obs.pointing_radec, width=2 * offset_max)\n",
    "\n",
    "        # A MapDataset is filled in this cutout geometry\n",
    "    dataset = maker.run(cutout, obs)\n",
    "        \n",
    "        # The data quality cut is applied\n",
    "    dataset = maker_safe_mask.run(dataset, obs)\n",
    "    \n",
    "    dataset = maker_bkg.run(dataset)\n",
    "        \n",
    "        # The resulting dataset cutout is stacked onto the final one\n",
    "    mapdataset_tot.stack(dataset)\n",
    "    \n",
    "    \n",
    "for obs in observations2:\n",
    "        # First a cutout of the target map is produced\n",
    "    cutout = mapdataset_dict[year].cutout(obs.pointing_radec, width=2 * offset_max)\n",
    "\n",
    "        # A MapDataset is filled in this cutout geometry\n",
    "    dataset = maker.run(cutout, obs)\n",
    "        \n",
    "        # The data quality cut is applied\n",
    "    dataset = maker_safe_mask.run(dataset, obs)\n",
    "    \n",
    "    dataset = maker_bkg.run(dataset)\n",
    "        \n",
    "        # The resulting dataset cutout is stacked onto the final one\n",
    "    mapdataset_tot.stack(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path = Path(\"$GAMMAPY_DATA/GC_variability2020/hap-hd\")\n",
    "#path.mkdir(exist_ok=True)\n",
    "\n",
    "filename = \"mapdataset_tot.fits.gz\"\n",
    "mapdataset_tot.write(path/filename, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "exclusion_region = RectangleSkyRegion(src_pos, 3*u.deg, 1*u.deg)\n",
    "exclusion_mask = geom.region_mask([exclusion_region], inside=False)\n",
    "exclusion_mask = Map.from_geom(geom, data=exclusion_mask)\n",
    "\n",
    "histo = dict()\n",
    "\n",
    "for year in range(2004,2020):\n",
    "    \n",
    "    offset_max = 2.0 * u.deg #on peut le réduire pour ne pas prendre les obs trop éloignées du centre de 2.0 deg à 1.8 deg\n",
    "    maker = MapDatasetMaker()\n",
    "    maker_safe_mask = SafeMaskMaker(methods=[\"offset-max\", \"bkg-peak\"], offset_max=offset_max)\n",
    "    maker_bkg = FoVBackgroundMaker(\"scale\", exclusion_mask)\n",
    "    \n",
    "\n",
    "    spectrum = PowerLaw2SpectralModel(index=2.3)\n",
    "\n",
    "    for obs in yearly_obs1[year]:\n",
    "        # First a cutout of the target map is produced\n",
    "        cutout = mapdataset_dict[year].cutout(obs.pointing_radec, width=2 * offset_max)\n",
    "\n",
    "        # A MapDataset is filled in this cutout geometry\n",
    "        dataset = maker.run(cutout, obs)\n",
    "        \n",
    "        # The data quality cut is applied\n",
    "        dataset = maker_safe_mask.run(dataset, obs)\n",
    "        \n",
    "        dataset = maker_bkg.run(dataset)\n",
    "        \n",
    "        # The resulting dataset cutout is stacked onto the final one\n",
    "        mapdataset_dict[year].stack(dataset)\n",
    "        \n",
    "    for obs in yearly_obs1u[year]:\n",
    "        # First a cutout of the target map is produced\n",
    "        cutout = mapdataset_dict[year].cutout(obs.pointing_radec, width=2 * offset_max)\n",
    "\n",
    "        # A MapDataset is filled in this cutout geometry\n",
    "        dataset = maker.run(cutout, obs)\n",
    "        \n",
    "        # The data quality cut is applied\n",
    "        dataset = maker_safe_mask.run(dataset, obs)\n",
    "        \n",
    "        dataset = maker_bkg.run(dataset)\n",
    "        \n",
    "        # The resulting dataset cutout is stacked onto the final one\n",
    "        mapdataset_dict[year].stack(dataset)\n",
    "        \n",
    "    for obs in yearly_obs2[year]:\n",
    "        # First a cutout of the target map is produced\n",
    "        cutout = mapdataset_dict[year].cutout(obs.pointing_radec, width=2 * offset_max)\n",
    "\n",
    "        # A MapDataset is filled in this cutout geometry\n",
    "        dataset = maker.run(cutout, obs)\n",
    "        \n",
    "        # The data quality cut is applied\n",
    "        dataset = maker_safe_mask.run(dataset, obs)\n",
    "\n",
    "        dataset = maker_bkg.run(dataset)\n",
    "        \n",
    "        # The resulting dataset cutout is stacked onto the final one\n",
    "        mapdataset_dict[year].stack(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(2004,2020):\n",
    "    \n",
    "    filename = \"mapdataset\" +str(year)+\".fits.gz\"\n",
    "    mapdataset_dict[year].write(path/filename, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting data on observation conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracing offsets of observations (relative to SgrA*) for each year\n",
    "\n",
    "sgra_pos = SkyCoord(359.94, -0.04, unit=\"deg\", frame=\"galactic\")\n",
    "\n",
    "histo_offset = dict()\n",
    "\n",
    "for year in range(2004,2020):\n",
    "    histo_offset[year] = []\n",
    "\n",
    "    for obs in yearly_obs1[year]:\n",
    "        direction = obs.pointing_radec\n",
    "        sep = direction.separation(sgra_pos) #calcule la séparation \"on sky\" entre les deux coordonnées\n",
    "        histo_offset[year].append(sep.value)\n",
    "        \n",
    "    for obs in yearly_obs1u[year]:\n",
    "        direction = obs.pointing_radec\n",
    "        sep = direction.separation(sgra_pos) #calcule la séparation \"on sky\" entre les deux coordonnées\n",
    "        histo_offset[year].append(sep.value)\n",
    "        \n",
    "    for obs in yearly_obs2[year]:\n",
    "        direction = obs.pointing_radec\n",
    "        sep = direction.separation(sgra_pos) #calcule la séparation \"on sky\" entre les deux coordonnées\n",
    "        histo_offset[year].append(sep.value)\n",
    "        \n",
    "    plt.figure()\n",
    "    plt.hist(histo_offset[year], 40, (0.0, 2.1))\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.xlabel(\"Décalage angulaire par rapport à la source (deg)\")\n",
    "    plt.ylabel(\"Nombre d'observations\")\n",
    "    \n",
    "    name = \"offsets/offsets_\"+str(year)+\".pdf\"\n",
    "    plt.savefig(path/, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracing zenithal angle of observation for each year\n",
    "\n",
    "histo_zen = dict()\n",
    "\n",
    "for year in range(2004,2020):  \n",
    "    histo_zen[year] = []\n",
    "    \n",
    "    obs_table_year1 = obs_table1.select_observations(year_intervals[year])\n",
    "    obs_table_year1u = obs_table1u.select_observations(year_intervals[year])\n",
    "    obs_table_year2 = obs_table2.select_observations(year_intervals[year])\n",
    "    res =  obs_table_year1[\"ZEN_PNT\"].tolist() + obs_table_year1u[\"ZEN_PNT\"].tolist() +obs_table_year2[\"ZEN_PNT\"].tolist() \n",
    "    \n",
    "    histo_zen[year] = res\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.hist(histo_zen[year], 40, (5.0, 65.0))\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.xlabel(\"Angle zénithal de l'observation (deg)\")\n",
    "    plt.ylabel(\"Nombre d'observations\")\n",
    "    \n",
    "    name = \"angzen/angzen_\"+str(year)+\".pdf\"\n",
    "    plt.savefig(path/name, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding up LiveTime for each year\n",
    "\n",
    "res = []\n",
    "\n",
    "for year in range(2004,2020):\n",
    "    obs_table_year1 = obs_table1.select_observations(year_intervals[year])\n",
    "    obs_table_year1u = obs_table1u.select_observations(year_intervals[year])\n",
    "    obs_table_year2 = obs_table2.select_observations(year_intervals[year])\n",
    "    \n",
    "    s = sum(obs_table_year1[\"LIVETIME\"].tolist())/3600 +sum(obs_table_year1u[\"LIVETIME\"].tolist())/3600 + sum(obs_table_year2[\"LIVETIME\"].tolist())/3600\n",
    "    res.append(s)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.plot(range(2004,2020), res)\n",
    "plt.grid()\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Total LIVETIME (hours)\")\n",
    "\n",
    "name = \"livetime_plot.pdf\"    \n",
    "plt.savefig(path/name, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
