{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation d'une source constante sauf une année"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "import astropy.units as u\n",
    "from astropy.coordinates import SkyCoord, Angle\n",
    "from astropy.convolution import Tophat2DKernel\n",
    "from regions import CircleSkyRegion, RectangleSkyRegion\n",
    "\n",
    "from astropy.time import Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.18.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from gammapy.detect import compute_lima_on_off_image,compute_lima_image # quelle différence entre les deux ?\n",
    "\n",
    "from gammapy.data import DataStore\n",
    "from gammapy.irf import PSFKernel\n",
    "from gammapy.maps import Map, MapAxis, WcsGeom\n",
    "from gammapy.datasets import MapDataset, Datasets, FluxPointsDataset\n",
    "from gammapy.makers import (\n",
    "    MapDatasetMaker,\n",
    "    SafeMaskMaker,\n",
    "    FoVBackgroundMaker,\n",
    ")\n",
    "from gammapy.modeling import Fit\n",
    "from gammapy.modeling.models import (\n",
    "    Models,\n",
    "    SkyModel,\n",
    "    BackgroundModel,\n",
    "    PowerLawSpectralModel,\n",
    "    PowerLaw2SpectralModel,\n",
    "    PointSpatialModel,\n",
    "    ExpCutoffPowerLawSpectralModel,\n",
    "    TemplateSpatialModel,\n",
    "    GaussianSpatialModel,\n",
    "    FoVBackgroundModel\n",
    ")\n",
    "#from gammapy.stats import significance, excess # utiles ?\n",
    "from gammapy.estimators import (\n",
    "    #LiMaMapEstimator,\n",
    "    TSMapEstimator,\n",
    "    ExcessMapEstimator,\n",
    "    FluxPointsEstimator\n",
    ")\n",
    "\n",
    "\n",
    "import gammapy\n",
    "gammapy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_pos = SkyCoord(359.94, -0.04, unit=\"deg\", frame=\"galactic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Directory for outputs\n",
    "\n",
    "path = Path(\"../../../hess_results/GC_variability_0.18.2/hap-fr\")\n",
    "path.mkdir(exist_ok=True)\n",
    "\n",
    "pathma = Path(path/\"mapdatasets\")\n",
    "pathma.mkdir(exist_ok=True)\n",
    "\n",
    "pathmo = Path(path/\"models\")\n",
    "pathmo.mkdir(exist_ok=True)\n",
    "\n",
    "# for consistency we will use the template using exp cutoff for both the central source and the DE\n",
    "# but it will generally require that the cutoff of the DE be frozen and set to infinity (lambda = 0)\n",
    "\n",
    "model_name = pathmo/\"models_template_2cutoff.yaml\" \n",
    "\n",
    "pathres = Path(path/\"simu_variable\")\n",
    "pathres.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Début de l'analyse (avec mapdatasets déjà écrits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création de la géométrie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "emin, emax = [0.5, 100] * u.TeV\n",
    "\n",
    "e_bins = 20\n",
    "\n",
    "energy_axis = MapAxis.from_energy_bounds(\n",
    "    emin.value, emax.value, e_bins, unit=\"TeV\"\n",
    ")\n",
    "geom = WcsGeom.create(\n",
    "    skydir=(0, 0),\n",
    "    binsz=0.02,\n",
    "    width=(10, 8),\n",
    "    frame=\"galactic\",\n",
    "    proj=\"CAR\",\n",
    "    axes=[energy_axis],\n",
    ")\n",
    "\n",
    "geom2d = WcsGeom.create(\n",
    "    skydir=(0, 0),\n",
    "    binsz=0.02,\n",
    "    width=(10, 8),\n",
    "    frame=\"galactic\",\n",
    "    proj=\"CAR\",\n",
    ")\n",
    "\n",
    "emintrue, emaxtrue = [0.3,200] * u.TeV\n",
    "e_bins_true = 30\n",
    "\n",
    "energy_axis_true = MapAxis.from_energy_bounds(\n",
    "    emintrue.value, emaxtrue.value, e_bins_true, unit=\"TeV\", name=\"energy_true\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_year = Datasets.read(pathma/\"datasets_year.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the model template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelGC,modelG09, modeldiff= Models.read(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed\n",
    "modelGC.parameters[\"amplitude\"].frozen = False\n",
    "modelGC.parameters[\"amplitude\"].value = 2.12e-12 #2.12\n",
    "\n",
    "modelGC.parameters[\"index\"].frozen = True\n",
    "modelGC.parameters[\"index\"].value = 1.77\n",
    "\n",
    "modelGC.spectral_model.parameters['lambda_'].frozen = True\n",
    "modelGC.spectral_model.parameters['lambda_'].value = 1/5.4\n",
    "\n",
    "modeldiff.parameters[\"amplitude\"].frozen = False\n",
    "modeldiff.parameters[\"amplitude\"].value = 6.09e-12\n",
    "\n",
    "modeldiff.parameters[\"index\"].frozen = True\n",
    "modeldiff.parameters[\"index\"].value = 2.24\n",
    "\n",
    "modeldiff.spectral_model.parameters['lambda_'].frozen = True\n",
    "modeldiff.spectral_model.parameters['lambda_'].value = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelGCnaive, modeldiffnaive = modelGC.copy(), modeldiff.copy()\n",
    "\n",
    "modelGCnaive.parameters[\"amplitude\"].value = 3e-12\n",
    "modeldiffnaive.parameters[\"amplitude\"].value = 5e-12\n",
    "\n",
    "# faire varier les modèles naifs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_region = RectangleSkyRegion(src_pos, 4*u.deg, 3*u.deg)\n",
    "\n",
    "J1745_303_region = CircleSkyRegion(SkyCoord(358.6,  -0.6, unit=\"deg\", frame=\"galactic\"), 0.75 * u.deg)\n",
    "\n",
    "mask_fit = geom.region_mask([fit_region])*geom.region_mask([J1745_303_region] , inside=False)\n",
    "\n",
    "mask_fit = Map.from_geom(geom, data=mask_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajustement du modèle aux cartes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_dataset(mapdataset):\n",
    "    \n",
    "    fit = Fit([mapdataset])\n",
    "    result = fit.run()\n",
    "    \n",
    "    table = result.parameters.to_table()\n",
    "    \n",
    "    #print(table)\n",
    "    \n",
    "    stat = fit.stat_profile(parameter='amplitude')\n",
    "    \n",
    "    #cov = result.parameters.covariance\n",
    "    \n",
    "    return table, result, stat# , cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajustement des données simulées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emin = 1.0*u.TeV\n",
    "emax = 10*u.TeV\n",
    "\n",
    "GCflux_distribution = {2004 : [] ,2005 : [] ,2006 : [] ,2007 : [] ,2008 : [] ,2009 : [] ,\n",
    "                       2010 : [] ,2011 : [] ,2012 : [] ,2013 : [] ,2014 : [] ,2015 : [] ,\n",
    "                       2016 : [] ,2017 : [] ,2018 : [] ,2019 : []  }\n",
    "DEflux_distribution = {2004 : [] ,2005 : [] ,2006 : [] ,2007 : [] ,2008 : [] ,2009 : [] ,\n",
    "                       2010 : [] ,2011 : [] ,2012 : [] ,2013 : [] ,2014 : [] ,2015 : [] ,\n",
    "                       2016 : [] ,2017 : [] ,2018 : [] ,2019 : []  }\n",
    "ExcessCounts =        {2004 : [] ,2005 : [] ,2006 : [] ,2007 : [] ,2008 : [] ,2009 : [] ,\n",
    "                       2010 : [] ,2011 : [] ,2012 : [] ,2013 : [] ,2014 : [] ,2015 : [] ,\n",
    "                       2016 : [] ,2017 : [] ,2018 : [] ,2019 : []  }\n",
    "\n",
    "# à ne définir qu'une fois, puis on rajoute des éléments \n",
    "#en prenant soin de laisser k augmenter sans remise à zéro dans les appels successifs de la cellule suivante\n",
    "\n",
    "k = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "k_0 = k\n",
    "\n",
    "while k < k_0 + 10 :\n",
    "    #ça fonctionne comme ça, tous les modèles existent indépendamment des datasets donc on peut continuer à \n",
    "    datasets_year = Datasets.read(pathma/\"datasets_year.yaml\")\n",
    "    \n",
    "    for year in range(2009,2010):\n",
    "        # données simulées\n",
    "        datasets_year[f'map_{year}'].models =  [modelGC.copy(),modelG09.copy(),modeldiff.copy()]\n",
    "        \n",
    "        datasets_year[f'map_{year}'].fake(k) \n",
    "        \n",
    "        #print(\"modèle pour le fake\",datasets_year[f\"map_{year}\"].models[0].parameters['amplitude'].value)\n",
    "        \n",
    "        \n",
    "        ExcessCounts[year].append(np.sum(datasets_year[f'map_{year}'].excess))\n",
    "        \n",
    "        # on remet les modèles\n",
    "        datasets_year[f'map_{year}'].mask_fit = mask_fit\n",
    "        bkg_model = FoVBackgroundModel(dataset_name=f\"map_{year}\")\n",
    "        \n",
    "        # partir de différentes valeur pour le fit pour chaque iteration\n",
    "        modelGCnaive.spectral_model.parameters['amplitude'].value = np.random.normal(4,1)*1e-12\n",
    "        \n",
    "        datasets_year[f'map_{year}'].models =  [modelGCnaive.copy(), modelG09.copy(), modeldiffnaive.copy(),bkg_model]\n",
    "        #print(\"modèle pour le fit\", datasets_year[f\"map_{year}\"].models[0].parameters['amplitude'].value)\n",
    "        \n",
    "        # on fait le fit\n",
    "        table, result, stat = fit_dataset(datasets_year[f\"map_{year}\"])\n",
    "        \n",
    "        #print(result)\n",
    "        \n",
    "        #plt.plot(stat['amplitude_scan'], stat['stat_scan'])\n",
    "        \n",
    "        #print(\"modèle fitté\", datasets_year[f\"map_{year}\"].models[0].parameters['amplitude'].value)\n",
    "        #print(\"erreur fittée\", datasets_year[f\"map_{year}\"].models[0].parameters['amplitude'].error)\n",
    "        #print(datasets_year[f\"map_{year}\"].models[0].parameters['amplitude'].error/datasets_year[f\"map_{year}\"].models[0].parameters['amplitude'].value)\n",
    "        \n",
    "        diffuse_flux = datasets_year[f'map_{year}'].models[2].spectral_model.integral(emin, emax)\n",
    "        GC_flux = datasets_year[f'map_{year}'].models[0].spectral_model.integral(emin, emax)\n",
    "        \n",
    "        GCflux_distribution[year].append(GC_flux.value)\n",
    "        DEflux_distribution[year].append(diffuse_flux.value)\n",
    "        \n",
    "    k = k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(GCflux_distribution[2009])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2004: [],\n",
       " 2005: [],\n",
       " 2006: [],\n",
       " 2007: [],\n",
       " 2008: [],\n",
       " 2009: [373.3882888692608,\n",
       "  306.3882888692608,\n",
       "  310.3882888692608,\n",
       "  486.3882888692608,\n",
       "  230.3882888692608,\n",
       "  314.3882888692608,\n",
       "  198.3882888692608,\n",
       "  280.3882888692608,\n",
       "  342.3882888692608,\n",
       "  158.3882888692608],\n",
       " 2010: [],\n",
       " 2011: [],\n",
       " 2012: [],\n",
       " 2013: [],\n",
       " 2014: [],\n",
       " 2015: [],\n",
       " 2016: [],\n",
       " 2017: [],\n",
       " 2018: [],\n",
       " 2019: []}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ExcessCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cutout() missing 2 required positional arguments: 'position' and 'width'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-190d42880c06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdatasets_year\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'map_2009'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcutout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_interactive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: cutout() missing 2 required positional arguments: 'position' and 'width'"
     ]
    }
   ],
   "source": [
    "datasets_year['map_2009'].excess.cutout().plot_interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we automatically exclude the years for which the fit didn't succeed (should be the same for all datasets here)\n",
    "\n",
    "years = []\n",
    "for year in range(2004,2020):\n",
    "    if results[year-2004].message =='Optimization terminated successfully.':\n",
    "        years.append(year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each year we will store the pvalue computed for a constant model, \n",
    "# knowing that for this year the intrinsic flux is 10%, 20%, 30%, 50% higher than the others years (effectively constant in terms of intrinsic flux)\n",
    "\n",
    "pvalue_dict = {2004 : [] ,2005 : [] ,2006 : [] ,2007 : [] ,2008 : [] ,2009 : [] ,\n",
    "               2010 : [] ,2011 : [] ,2012 : [] ,2013 : [] ,2014 : [] ,2015 : [] ,\n",
    "               2016 : [] ,2017 : [] ,2018 : [] ,2019 : []  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    for year in years:\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time evolution of the fluxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalculateFluxFromModels(mapdataset, emin, emax):\n",
    "    models  = mapdataset.models\n",
    "    \n",
    "    amplitudeGC = models[0].spectral_model.parameters['amplitude'].value\n",
    "    amp_errGC = models[0].spectral_model.parameters['amplitude'].error\n",
    "    \n",
    "    amplitudediff = models[2].spectral_model.parameters['amplitude'].value\n",
    "    amp_errdiff = models[2].spectral_model.parameters['amplitude'].error\n",
    "    \n",
    "    #norm = mapdataset.background_model.parameters['norm'].value\n",
    "    #norm_err = mapdataset.background_model.parameters['norm'].error\n",
    "    \n",
    "    if isinstance(emin, u.Quantity):\n",
    "\n",
    "        diffuse_flux = models[2].spectral_model.integral(emin, emax)\n",
    "        GC_flux = models[0].spectral_model.integral(emin, emax)\n",
    "        \n",
    "    if np.isscalar(emin):\n",
    "        emin = emin*u.TeV\n",
    "        emax = emax*u.TeV\n",
    "        diffuse_flux = models[2].spectral_model.integral(emin, emax)\n",
    "        GC_flux = models[0].spectral_model.integral(emin, emax)\n",
    "\n",
    "    return diffuse_flux, GC_flux, amplitudeGC, amp_errGC, amplitudediff, amp_errdiff#, norm, norm_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting parameters from each years fitted model\n",
    "\n",
    "emin = 1.0*u.TeV\n",
    "emax = 10*u.TeV\n",
    "resGC = []\n",
    "resdiff = []\n",
    "ampsGC = []\n",
    "amp_errsGC = []\n",
    "ampsdiff = []\n",
    "amp_errsdiff = []\n",
    "#norms = []\n",
    "#norms_err = []\n",
    "\n",
    "\n",
    "for year in years:\n",
    "    dif , GC, amp, amp_err, ampdiff, amp_errdiff = CalculateFluxFromModels(datasets_year[f\"map_{year}\"] , emin, emax)\n",
    "    \n",
    "    resGC.append(GC.value)\n",
    "    resdiff.append(dif.value)\n",
    "    ampsGC.append(amp)\n",
    "    amp_errsGC.append(amp_err)\n",
    "    ampsdiff.append(ampdiff)\n",
    "    amp_errsdiff.append(amp_errdiff)\n",
    "   \n",
    "    \n",
    "    #norms.append(norm)\n",
    "    #norms_err.append(norm_err)\n",
    "    \n",
    "resGC = np.asarray(resGC)\n",
    "ampsGC = np.asarray(ampsGC)\n",
    "amp_errsGC = np.asarray(amp_errsGC)\n",
    "resdiff = np.asarray(resdiff)\n",
    "ampsdiff = np.asarray(ampsdiff)\n",
    "amp_errsdiff = np.asarray(amp_errsdiff)\n",
    "#norms = np.asarray(norms)\n",
    "#norms_err = np.asarray(norms_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Errors computation\n",
    "\n",
    "yerrGC = (resGC/ampsGC)*amp_errsGC\n",
    "yerrdiff = (resdiff/ampsdiff)*amp_errsdiff\n",
    "\n",
    "cross_term = []\n",
    "\n",
    "#cov_per_year n'existe plus ???\n",
    "#1,8,13,17\n",
    "\n",
    "for k,year in zip(range(len(years)),years):\n",
    "    term = 2*(datasets_year[f\"map_{year}\"].models.covariance.data[1,13]*yerrGC[k]*yerrdiff[k])/(resGC[k]*resdiff[k])\n",
    "    cross_term.append(term)\n",
    "      \n",
    "yerr_rap = (resGC/resdiff)*np.sqrt((yerrGC/resGC)**2 + (yerrdiff/resdiff)**2 - cross_term )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a time evolution model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "from astropy.modeling import models, fitting\n",
    "\n",
    "from scipy.stats import chisquare, chi2\n",
    "\n",
    "# chi2 non réduit\n",
    "def chisq(obs, exp, error):\n",
    "    chisq = 0\n",
    "    for i in range(len(obs)):\n",
    "        chisq = chisq + ((obs[i]-exp)**2)/(error[i]**2)\n",
    "    return chisq\n",
    "\n",
    "# chi2 réduit\n",
    "def chisqr(obs, exp, error):\n",
    "    chisqr = 0\n",
    "    for i in range(len(obs)):\n",
    "        chisqr = chisqr + ((obs[i]-exp)**2)/(error[i]**2)\n",
    "    return chisqr/(len(obs) -1)\n",
    "\n",
    "def pval(obs, exp, error, nddl): # number of DoF is the number of points minus number of fitted parameters (1 for a constant)\n",
    "    chisq = 0\n",
    "    for i in range(len(obs)):\n",
    "        chisq = chisq + ((obs[i]-exp)**2)/(error[i]**2)\n",
    "    pval = 1 - chi2.cdf(chisq, nddl)\n",
    "    return pval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolution of the ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = years\n",
    "y = resGC/resdiff\n",
    "y_uncs = yerr_rap\n",
    "\n",
    "\n",
    "# Fit the data using a box model.\n",
    "# Bounds are not really needed but included here to demonstrate usage.\n",
    "t_init = models.Const1D(0.6)\n",
    "fit_t = fitting.LevMarLSQFitter()\n",
    "t = fit_t(t_init, x, y, weights=1.0/y_uncs)\n",
    "\n",
    "\n",
    "A1 = y\n",
    "A2 = t(2004)\n",
    "err_bars = yerr_rap\n",
    "\n",
    "pv = pval(A1, A2, err_bars, len(A1)-1)\n",
    "\n",
    "error_const = np.sqrt(sum([a**2 for a in err_bars]))/len(A1)\n",
    "\n",
    "\n",
    "# Plot the data with the best-fit model\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(x, y, 'ko')\n",
    "plt.errorbar(x, y, yerr=y_uncs, fmt='kx', label=\"data\")\n",
    "plt.plot(x, t(x), label=f\"constant ratio = {A2:0.2e} ± {error_const:0.1e}, pval = {pv:0.3f}\")\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('')\n",
    "plt.legend(loc=1)\n",
    "\n",
    "plt.title(\"Fitting of the source/DE ratio\")\n",
    "plt.savefig(pathres/\"LC_ratio_1cut_fr.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathm = Path(pathres/\"model_maps\")\n",
    "pathm.mkdir(exist_ok=True)\n",
    "\n",
    "for year in years:\n",
    "    plt.figure(figsize=(5,5))\n",
    "    datasets_year[f\"map_{year}\"].npred().sum_over_axes().cutout(src_pos,3*u.deg).smooth('0.05 deg').plot()\n",
    "    plt.title(\"model prediction (npred) \" + str(year))\n",
    "    \n",
    "    name =\"model_\"+ str(year)+\"_map_fr.pdf\"\n",
    "    plt.savefig(pathm/name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathr = Path(pathres/\"residuals\")\n",
    "pathr.mkdir(exist_ok=True)\n",
    "\n",
    "kwargs_spatial = {'vmin':-1, 'vmax':1 } # nécessaire ?\n",
    "\n",
    "kwargs_spectral = {'method':'diff/sqrt(model)', 'region' : CircleSkyRegion(\n",
    "    center=src_pos, radius=0.5 * u.deg\n",
    ")}\n",
    "\n",
    "\n",
    "for year in years:\n",
    "\n",
    "    datasets_year[f\"map_{year}\"].plot_residuals(kwargs_spatial=kwargs_spatial, kwargs_spectral=kwargs_spectral)\n",
    "    name = \"residuals_\"+str(year)+\"_fr_full.pdf\"\n",
    "    plt.title(\"residuals (diff/sqrt(model)) \" + str(year))\n",
    "    plt.savefig(pathr/name, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#très long ?\n",
    "\n",
    "paths = Path(pathres/\"significance\")\n",
    "paths.mkdir(exist_ok=True)\n",
    "\n",
    "lima_significances = dict()\n",
    "\n",
    "for year in years:\n",
    "    plt.figure()\n",
    "\n",
    "    lima_estimator = ExcessMapEstimator(\"0.2 deg\", selection_optional=[] )\n",
    "    \n",
    "    lima_significances[year] = lima_estimator.run(datasets_year[f\"map_{year}\"])\n",
    "    \n",
    "    lima_significances[year]['sqrt_ts'].plot(add_cbar=True)\n",
    "    name = \"significance_\"+str(year)+\"_fr_full.pdf\"\n",
    "    \n",
    "    plt.title(\"Significance map (data v. predictions) \"+str(year))\n",
    "    plt.savefig(paths/name, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significance distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "pathh = Path(paths/\"significance_distrib\")\n",
    "pathh.mkdir(exist_ok=True)\n",
    "\n",
    "for year in years:\n",
    "    plt.figure()\n",
    "    signidata = lima_significances[year]['sqrt_ts'].cutout(position=src_pos, width=(3 * u.deg, 1.5* u.deg)).data\n",
    "    \n",
    "    # on masque la région en bas à droite\n",
    "    mask = fit_mask.reduce_over_axes(func=np.logical_or).cutout(position=src_pos, width=(3 * u.deg, 1.5* u.deg)).data\n",
    "    \n",
    "    plt.hist(signidata[0,mask].flatten(),30, histtype='step', density=True)\n",
    "    \n",
    "    mean,std=norm.fit(signidata[0,mask])\n",
    "    xmin, xmax = plt.xlim()\n",
    "    x = np.linspace(xmin, xmax, 100)\n",
    "    y = norm.pdf(x, mean, std)\n",
    "    plt.plot(x, y, label=r'$\\mu$ = {0:03.4f}, $\\sigma$ = {1:03.4f}'.format(mean,std))\n",
    "    y2= norm.pdf(x, 0, 1)   \n",
    "    plt.plot(x, y2 ,label=r'$\\mu$ = 0, $\\sigma$ = 1')\n",
    "    \n",
    "    plt.legend()\n",
    "    \n",
    "    name = \"significance_hist\"+str(year)+\"_fr_full.pdf\"\n",
    "    plt.title(\"Significance distribution on the central (3°,1.5°) \"+str(year))\n",
    "    plt.savefig(pathh/name, overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
