{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data reduction for the hap-fr channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "import astropy.units as u\n",
    "from astropy.coordinates import SkyCoord,Angle\n",
    "from astropy.convolution import Tophat2DKernel\n",
    "from regions import CircleSkyRegion, RectangleSkyRegion\n",
    "\n",
    "from astropy.time import Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.18.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from gammapy.detect import compute_lima_on_off_image\n",
    "\n",
    "from gammapy.data import DataStore\n",
    "from gammapy.irf import PSFKernel\n",
    "from gammapy.maps import Map, MapAxis, WcsGeom\n",
    "from gammapy.datasets import MapDataset, Datasets\n",
    "from gammapy.makers import (\n",
    "    MapDatasetMaker,\n",
    "    SafeMaskMaker,\n",
    "    FoVBackgroundMaker,\n",
    ")\n",
    "\n",
    "\n",
    "from gammapy.modeling.models import (\n",
    "    SkyModel,\n",
    "    BackgroundModel,\n",
    "    PowerLawSpectralModel,\n",
    "    PowerLaw2SpectralModel,\n",
    "    PointSpatialModel,\n",
    "    ExpCutoffPowerLawSpectralModel,\n",
    "    TemplateSpatialModel\n",
    ")\n",
    "from gammapy.modeling import Fit\n",
    "\n",
    "\n",
    "import gammapy\n",
    "gammapy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_pos = SkyCoord(359.94, -0.04, unit=\"deg\", frame=\"galactic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path = Path(\"../../../hess_results/GC_variability_0.18.2/hap-fr\")\n",
    "path.mkdir(exist_ok=True)\n",
    "\n",
    "pathm = Path(path/\"mapdatasets\")\n",
    "pathm.mkdir(exist_ok=True)\n",
    "\n",
    "patho = Path(path/\"offsets\")\n",
    "patho.mkdir(exist_ok=True)\n",
    "\n",
    "pathz = Path(path/\"angzen\")\n",
    "pathz.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data store:\n",
      "HDU index table:\n",
      "BASE_DIR: /home/samuel/code/gammapy_data/ash_stereo_Prod17_Calib0834_thsq64\n",
      "Rows: 127056\n",
      "OBS_ID: 18092 -- 154591\n",
      "HDU_TYPE: ['aeff', 'bkg', 'edisp', 'events', 'gti', 'psf']\n",
      "HDU_CLASS: ['aeff_2d', 'bkg_2d', 'edisp_2d', 'events', 'gti', 'psf_table']\n",
      "\n",
      "\n",
      "Observation table:\n",
      "Observatory name: 'N/A'\n",
      "Number of observations: 21186\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_store = DataStore.from_dir(\"$GAMMAPY_DATA/ash_stereo_Prod17_Calib0834_thsq64\")\n",
    "data_store.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correcting background file extensions for HAP-fr data (to be done once)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sky region selection : observation pointing within this region will be selected \n",
    "#(this can be redundant with maximum offset selection, but using the max_offset should be prefered)\n",
    "\n",
    "from astropy.coordinates import Angle\n",
    "\n",
    "selection = dict(type='sky_circle', frame='galactic',\n",
    "                 lon=Angle(0, 'deg'),\n",
    "                 lat=Angle(0, 'deg'),\n",
    "                 radius=Angle(1.8, 'deg'),\n",
    "                 border=Angle(0, 'deg'))\n",
    "\n",
    "\n",
    "# selecting the admitted range of zenithal angle, such selection can be done for any column of the obs_table\n",
    "selectionZEN = dict(type='par_box', variable='ZEN_PNT', value_range=Angle([0.,50.], 'deg'))\n",
    "\n",
    "obs_table = data_store.obs_table.select_observations(selection)\n",
    "obs_table = obs_table.select_observations(selectionZEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No HDU found matching: OBS_ID = 31539, HDU_TYPE = aeff, HDU_CLASS = None\n",
      "No HDU found matching: OBS_ID = 31539, HDU_TYPE = edisp, HDU_CLASS = None\n",
      "No HDU found matching: OBS_ID = 31539, HDU_TYPE = psf, HDU_CLASS = None\n",
      "No HDU found matching: OBS_ID = 31577, HDU_TYPE = aeff, HDU_CLASS = None\n",
      "No HDU found matching: OBS_ID = 31577, HDU_TYPE = edisp, HDU_CLASS = None\n",
      "No HDU found matching: OBS_ID = 31577, HDU_TYPE = psf, HDU_CLASS = None\n",
      "No HDU found matching: OBS_ID = 31578, HDU_TYPE = aeff, HDU_CLASS = None\n",
      "No HDU found matching: OBS_ID = 31578, HDU_TYPE = edisp, HDU_CLASS = None\n",
      "No HDU found matching: OBS_ID = 31578, HDU_TYPE = psf, HDU_CLASS = None\n",
      "No HDU found matching: OBS_ID = 31579, HDU_TYPE = aeff, HDU_CLASS = None\n",
      "No HDU found matching: OBS_ID = 31579, HDU_TYPE = edisp, HDU_CLASS = None\n",
      "No HDU found matching: OBS_ID = 31579, HDU_TYPE = psf, HDU_CLASS = None\n",
      "No HDU found matching: OBS_ID = 31580, HDU_TYPE = aeff, HDU_CLASS = None\n",
      "No HDU found matching: OBS_ID = 31580, HDU_TYPE = edisp, HDU_CLASS = None\n",
      "No HDU found matching: OBS_ID = 31580, HDU_TYPE = psf, HDU_CLASS = None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed observation : 129467\n",
      "Removed observation : 129471\n",
      "Removed observation : 153792\n",
      "Removed observation : 146971\n",
      "Removed observation : 147119\n",
      "Removed observation : 151790\n",
      "Removed observation : 153030\n",
      "Removed observation : 153031\n",
      "Removed observation : 153791\n",
      "Removed observation : 153793\n",
      "Removed observation : 153794\n",
      "Removed observation : 153810\n",
      "Removed observation : 153811\n",
      "Removed observation : 153812\n",
      "Removed observation : 153813\n",
      "Removed observation : 153836\n",
      "Removed observation : 153837\n",
      "Removed observation : 153838\n",
      "Removed observation : 153839\n",
      "Removed observation : 153840\n",
      "Removed observation : 153841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No HDU found matching: OBS_ID = 31539, HDU_TYPE = aeff, HDU_CLASS = None\n",
      "No HDU found matching: OBS_ID = 31539, HDU_TYPE = edisp, HDU_CLASS = None\n",
      "No HDU found matching: OBS_ID = 31539, HDU_TYPE = psf, HDU_CLASS = None\n",
      "No HDU found matching: OBS_ID = 31577, HDU_TYPE = aeff, HDU_CLASS = None\n",
      "No HDU found matching: OBS_ID = 31577, HDU_TYPE = edisp, HDU_CLASS = None\n",
      "No HDU found matching: OBS_ID = 31577, HDU_TYPE = psf, HDU_CLASS = None\n",
      "No HDU found matching: OBS_ID = 31578, HDU_TYPE = aeff, HDU_CLASS = None\n",
      "No HDU found matching: OBS_ID = 31578, HDU_TYPE = edisp, HDU_CLASS = None\n",
      "No HDU found matching: OBS_ID = 31578, HDU_TYPE = psf, HDU_CLASS = None\n",
      "No HDU found matching: OBS_ID = 31579, HDU_TYPE = aeff, HDU_CLASS = None\n",
      "No HDU found matching: OBS_ID = 31579, HDU_TYPE = edisp, HDU_CLASS = None\n",
      "No HDU found matching: OBS_ID = 31579, HDU_TYPE = psf, HDU_CLASS = None\n",
      "No HDU found matching: OBS_ID = 31580, HDU_TYPE = aeff, HDU_CLASS = None\n",
      "No HDU found matching: OBS_ID = 31580, HDU_TYPE = edisp, HDU_CLASS = None\n",
      "No HDU found matching: OBS_ID = 31580, HDU_TYPE = psf, HDU_CLASS = None\n"
     ]
    }
   ],
   "source": [
    "# Removing observations lacking IRFs (effective area, energy dispersion, point spread function and background model)\n",
    "\n",
    "ids = obs_table[\"OBS_ID\"].tolist()\n",
    "observations = data_store.get_observations(ids, skip_missing=True)\n",
    "\n",
    "for obs in observations:\n",
    "    try:\n",
    "        obs.aeff\n",
    "        obs.edisp\n",
    "        obs.psf\n",
    "        obs.bkg\n",
    "            \n",
    "    except:\n",
    "        ids.remove(obs.obs_id)\n",
    "        print(\"Removed observation : \" + str(obs.obs_id))\n",
    "        \n",
    "observations = data_store.get_observations(ids, skip_missing=True)\n",
    "obs_table = obs_table.select_obs_id(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time intervals used for selection\n",
    "\n",
    "t2004  = dict(type='time_box', time_range= Time(['2004-01-01T00:00:00', '2004-12-31T23:59:59']))\n",
    "t2005  = dict(type='time_box', time_range= Time(['2005-01-01T00:00:00', '2005-12-31T23:59:59']))\n",
    "t2006  = dict(type='time_box', time_range= Time(['2006-01-01T00:00:00', '2006-12-31T23:59:59']))\n",
    "t2007  = dict(type='time_box', time_range= Time(['2007-01-01T00:00:00', '2007-12-31T23:59:59']))\n",
    "t2008  = dict(type='time_box', time_range= Time(['2008-01-01T00:00:00', '2008-12-31T23:59:59']))\n",
    "t2009  = dict(type='time_box', time_range= Time(['2009-01-01T00:00:00', '2009-12-31T23:59:59']))\n",
    "t2010  = dict(type='time_box', time_range= Time(['2010-01-01T00:00:00', '2010-12-31T23:59:59']))\n",
    "t2011  = dict(type='time_box', time_range= Time(['2011-01-01T00:00:00', '2011-12-31T23:59:59']))\n",
    "t2012  = dict(type='time_box', time_range= Time(['2012-01-01T00:00:00', '2012-12-31T23:59:59']))\n",
    "t2013  = dict(type='time_box', time_range= Time(['2013-01-01T00:00:00', '2013-12-31T23:59:59']))\n",
    "t2014  = dict(type='time_box', time_range= Time(['2014-01-01T00:00:00', '2014-12-31T23:59:59']))\n",
    "t2015  = dict(type='time_box', time_range= Time(['2015-01-01T00:00:00', '2015-12-31T23:59:59']))\n",
    "t2016  = dict(type='time_box', time_range= Time(['2016-01-01T00:00:00', '2016-12-31T23:59:59']))\n",
    "t2017  = dict(type='time_box', time_range= Time(['2017-01-01T00:00:00', '2017-12-31T23:59:59']))\n",
    "t2018  = dict(type='time_box', time_range= Time(['2018-01-01T00:00:00', '2018-12-31T23:59:59']))\n",
    "t2019  = dict(type='time_box', time_range= Time(['2019-01-01T00:00:00', '2019-12-31T23:59:59']))\n",
    "\n",
    "year_intervals = { 2004 : t2004, 2005 : t2005, 2006 : t2006, 2007 : t2007,\n",
    "                      2008 : t2008, 2009 : t2009, 2010 : t2010, 2011 : t2011,\n",
    "                      2012 : t2012, 2013 : t2013, 2014 : t2014, 2015 : t2015,\n",
    "                      2016 : t2016, 2017 : t2017, 2018 : t2018, 2019 : t2019}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the geometry for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "emin, emax = [0.5, 100] * u.TeV\n",
    "\n",
    "energy_axis = MapAxis.from_energy_bounds(\n",
    "    emin.value, emax.value, 20, unit=\"TeV\"\n",
    ")\n",
    "geom = WcsGeom.create(\n",
    "    skydir=(0, 0),\n",
    "    binsz=0.02,\n",
    "    width=(10, 8),\n",
    "    frame=\"galactic\",\n",
    "    proj=\"CAR\",\n",
    "    axes=[energy_axis],\n",
    ")\n",
    "\n",
    "energy_axis_true = MapAxis.from_energy_bounds(0.3, 200, 30, unit=\"TeV\", name=\"energy_true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the MapDataset objects\n",
    "\n",
    "stacked = MapDataset.create(geom=geom, energy_axis_true=energy_axis_true, name=\"stacked\")\n",
    "\n",
    "\n",
    "stacked_period = Datasets([MapDataset.create(geom=geom, energy_axis_true=energy_axis_true, name=\"hess1\"),\n",
    "                           MapDataset.create(geom=geom, energy_axis_true=energy_axis_true, name=\"hess1u\"),\n",
    "                           MapDataset.create(geom=geom, energy_axis_true=energy_axis_true, name=\"hess2\")])\n",
    "\n",
    "datasets_year = Datasets()\n",
    "\n",
    "for k in range (2004,2020):\n",
    "    name = f\"map_{k}\"\n",
    "    datasets_year.append(MapDataset.create(geom=geom, energy_axis_true=energy_axis_true, name=name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets for each year (and the complete one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusion_region = RectangleSkyRegion(src_pos, 3*u.deg, 1*u.deg) # J1745-303 ?\n",
    "exclusion_mask = geom.region_mask([exclusion_region], inside=False)\n",
    "exclusion_mask = Map.from_geom(geom, data=exclusion_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_max = 1.8 * u.deg\n",
    "maker = MapDatasetMaker()\n",
    "maker_safe_mask = SafeMaskMaker(methods=[\"offset-max\", \"bkg-peak\"], offset_max=offset_max)\n",
    "maker_bkg = FoVBackgroundMaker(\"scale\", exclusion_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histo_thresh = dict()\n",
    "#for k in range(2004,2020): histo_thresh[k] = []\n",
    "\n",
    "#histo_thresh_tot = []\n",
    "#histo_thresh1 = []\n",
    "#histo_thresh2 = []\n",
    "#histo_thresh1u = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: VerifyWarning: Invalid keyword for column 8: Column null option (TNULLn) is invalid for binary table columns of type '1E' (got -1).  The invalid value will be ignored for the purpose of formatting the data in this column. [astropy.io.fits.column]\n",
      "WARNING: VerifyWarning: Invalid keyword for column 21: Column null option (TNULLn) is invalid for binary table columns of type '1E' (got -1).  The invalid value will be ignored for the purpose of formatting the data in this column. [astropy.io.fits.column]\n",
      "WARNING: VerifyWarning: Invalid keyword for column 23: Column null option (TNULLn) is invalid for binary table columns of type '1E' (got -1).  The invalid value will be ignored for the purpose of formatting the data in this column. [astropy.io.fits.column]\n",
      "WARNING: VerifyWarning: Invalid keyword for column 25: Column null option (TNULLn) is invalid for binary table columns of type '1E' (got -1).  The invalid value will be ignored for the purpose of formatting the data in this column. [astropy.io.fits.column]\n",
      "WARNING: VerifyWarning: Invalid keyword for column 26: Column null option (TNULLn) is invalid for binary table columns of type '1D' (got -1).  The invalid value will be ignored for the purpose of formatting the data in this column. [astropy.io.fits.column]\n",
      "WARNING: VerifyWarning: Invalid keyword for column 27: Column null option (TNULLn) is invalid for binary table columns of type '1E' (got -1).  The invalid value will be ignored for the purpose of formatting the data in this column. [astropy.io.fits.column]\n",
      "WARNING: VerifyWarning: Invalid keyword for column 28: Column null option (TNULLn) is invalid for binary table columns of type '1E' (got -1).  The invalid value will be ignored for the purpose of formatting the data in this column. [astropy.io.fits.column]\n",
      "WARNING: VerifyWarning: Invalid keyword for column 29: Column null option (TNULLn) is invalid for binary table columns of type '1E' (got -1).  The invalid value will be ignored for the purpose of formatting the data in this column. [astropy.io.fits.column]\n",
      "WARNING: VerifyWarning: Invalid keyword for column 30: Column null option (TNULLn) is invalid for binary table columns of type '1E' (got -1).  The invalid value will be ignored for the purpose of formatting the data in this column. [astropy.io.fits.column]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input given for unknown axis: {'energy': <Quantity [[[  0.5       ]],\n\n           [[  0.65166066]],\n\n           [[  0.84932323]],\n\n           [[  1.10694108]],\n\n           [[  1.44269991]],\n\n           [[  1.88030155]],\n\n           [[  2.45063709]],\n\n           [[  3.19396757]],\n\n           [[  4.16276604]],\n\n           [[  5.42542173]],\n\n           [[  7.07106781]],\n\n           [[  9.21587344]],\n\n           [[ 12.01124434]],\n\n           [[ 15.65451083]],\n\n           [[ 20.40285773]],\n\n           [[ 26.59147948]],\n\n           [[ 34.65724216]],\n\n           [[ 45.16952262]],\n\n           [[ 58.87040187]],\n\n           [[ 76.7270499 ]],\n\n           [[100.        ]]] TeV>}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gammapy-0.18.2/lib/python3.7/site-packages/gammapy/makers/map.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, dataset, observation)\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"background\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselection\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             kwargs[\"background\"] = self.make_background(\n\u001b[0;32m--> 294\u001b[0;31m                 \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             )\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gammapy-0.18.2/lib/python3.7/site-packages/gammapy/makers/map.py\u001b[0m in \u001b[0;36mmake_background\u001b[0;34m(self, geom, observation)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mbkg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbkg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0mgeom\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgeom\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0moversampling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackground_oversampling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         )\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gammapy-0.18.2/lib/python3.7/site-packages/gammapy/makers/utils.py\u001b[0m in \u001b[0;36mmake_map_background_irf\u001b[0;34m(pointing, ontime, bkg, geom, oversampling)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mfov_lon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfov_lon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mfov_lat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfov_lat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0menergy_reco\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menergies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m     )\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gammapy-0.18.2/lib/python3.7/site-packages/gammapy/irf/background.py\u001b[0m in \u001b[0;36mevaluate_integrate\u001b[0;34m(self, fov_lon, fov_lat, energy_reco, method)\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0mD\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0maxes\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \"\"\"\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfov_lon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfov_lat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menergy_reco\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrapz_loglog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menergy_reco\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gammapy-0.18.2/lib/python3.7/site-packages/gammapy/irf/background.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, fov_lon, fov_lat, energy_reco, method, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfov_lon\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfov_lat\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         return self.data.evaluate(\n\u001b[0;32m--> 363\u001b[0;31m             \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menergy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menergy_reco\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         )\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gammapy-0.18.2/lib/python3.7/site-packages/gammapy/utils/nddata.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, method, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# This is to catch e.g. typos in axis names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Input given for unknown axis: {kwargs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_regular_grid_interp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input given for unknown axis: {'energy': <Quantity [[[  0.5       ]],\n\n           [[  0.65166066]],\n\n           [[  0.84932323]],\n\n           [[  1.10694108]],\n\n           [[  1.44269991]],\n\n           [[  1.88030155]],\n\n           [[  2.45063709]],\n\n           [[  3.19396757]],\n\n           [[  4.16276604]],\n\n           [[  5.42542173]],\n\n           [[  7.07106781]],\n\n           [[  9.21587344]],\n\n           [[ 12.01124434]],\n\n           [[ 15.65451083]],\n\n           [[ 20.40285773]],\n\n           [[ 26.59147948]],\n\n           [[ 34.65724216]],\n\n           [[ 45.16952262]],\n\n           [[ 58.87040187]],\n\n           [[ 76.7270499 ]],\n\n           [[100.        ]]] TeV>}"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for obs in observations:\n",
    "    #print(f\"Processing obs ID: {obs.obs_id}\")\n",
    "    cutout = stacked.cutout(obs.pointing_radec, width=2.0 * offset_max)\n",
    "\n",
    "    # A MapDataset is filled in this cutout geometry\n",
    "    dataset = maker.run(cutout, obs)\n",
    "        \n",
    "    # The data quality cut is applied\n",
    "    dataset = maker_safe_mask.run(dataset, obs)\n",
    "        \n",
    "    dataset = maker_bkg.run(dataset)\n",
    "       \n",
    "    year = dataset.gti.time_start.jyear.astype('int')[0]\n",
    "    norm_bkg = dataset.background_model.spectral_model.norm.value\n",
    "    \n",
    "    if np.abs(norm_bkg - 1.) <= 0.3:\n",
    "        \n",
    "        datasets_year[f\"map_{year}\"].stack(dataset)\n",
    "        stacked.stack(dataset)\n",
    "        \n",
    "        # Attempting to save the energy threshold (not working)\n",
    "        #i = 0\n",
    "        #threshold = np.max(dataset.mask_safe.data[i, :,:])\n",
    "        \n",
    "        #while threshold == False:\n",
    "        #   i+=1\n",
    "        #    threshold = np.max(dataset.mask_safe.data[0, :,:])\n",
    "            \n",
    "        #histo_thresh[year].append(energy_axis.edges[i].value)\n",
    "        #histo_thresh_tot.append(energy_axis.edges[i].value)\n",
    "        \n",
    "        if (obs.obs_id > 20190 and obs.obs_id < 85000) or (obs.obs_id > 85294 and obs.obs_id < 85393):\n",
    "            stacked_period[\"hess1\"].stack(dataset)\n",
    "            #histo_thresh1.append(energy_axis.edges[i].value)\n",
    "                \n",
    "        if (obs.obs_id > 85392 and obs.obs_id < 123635) or (obs.obs_id > 84999 and obs.obs_id < 85293):\n",
    "            stacked_period[\"hess2\"].stack(dataset)\n",
    "            #histo_thresh2.append(energy_axis.edges[i].value)\n",
    "                \n",
    "        if (obs.obs_id > 129418 and obs.obs_id < 153843):\n",
    "            stacked_period[\"hess1u\"].stack(dataset)\n",
    "            #histo_thresh1u.append(energy_axis.edges[i].value)\n",
    "    #else:\n",
    "    #    print(f\"Bkg norm is out of bounds: {norm_bkg:.2f}. Rejecting observation {obs.obs_id}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"mapdataset_tot.fits.gz\"\n",
    "stacked.write(pathm/filename, overwrite=True)\n",
    "\n",
    "stacked_period.write(pathm/\"datasets_period.yaml\", overwrite=True)\n",
    "\n",
    "datasets_year.write(pathm/\"datasets_year.yaml\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting data on observation conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offset angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracing offsets of observations (relative to SgrA*) for each year\n",
    "\n",
    "sgra_pos = SkyCoord(359.94, -0.04, unit=\"deg\", frame=\"galactic\")\n",
    "\n",
    "histo_offset = dict()\n",
    "offset_tot = []\n",
    "offset_hess1 = []\n",
    "offset_hess2 = []\n",
    "offset_hess1u = []\n",
    "\n",
    "for year in range(2004,2020):\n",
    "    histo_offset[year] = []\n",
    "\n",
    "for obs in observations:\n",
    "    year = obs.gti.time_start.jyear.astype('int')[0]\n",
    "    \n",
    "    direction = obs.pointing_radec\n",
    "    sep = direction.separation(sgra_pos) #calcule la séparation \"on sky\" entre les deux coordonnées\n",
    "    \n",
    "    histo_offset[year].append(sep.value)\n",
    "    offset_tot.append(sep.value)\n",
    "        \n",
    "        \n",
    "for year in range(2004,2020):        \n",
    "    plt.figure()\n",
    "    plt.hist(histo_offset[year], 40, (0.0, 2.1))\n",
    "    \n",
    "    plt.title(str(year))\n",
    "    plt.grid()\n",
    "    plt.xlabel(\"Offset with respect to the central source (deg)\")\n",
    "    plt.ylabel(\"Number of observations\")\n",
    "    \n",
    "    name = \"offsets_\"+str(year)+\"_hd.pdf\"\n",
    "    plt.savefig(patho/name, overwrite=True)\n",
    "    \n",
    "\n",
    "plt.figure()\n",
    "plt.hist(offset_tot, 40, (0.0, 2.1))\n",
    "    \n",
    "plt.title(\"Total (2004-2019)\")\n",
    "plt.grid()\n",
    "plt.xlabel(\"Offset with respect to the central source(deg)\")\n",
    "plt.ylabel(\"Number of observations\")\n",
    "name = 'offset_tot_hd.pdf'\n",
    "plt.savefig(patho/name, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(offset_hess1, 40, (0.0, 2.1))\n",
    "    \n",
    "plt.title(\"Total HESS 1\")\n",
    "plt.grid()\n",
    "plt.xlabel(\"Offset with respect to the central source(deg)\")\n",
    "plt.ylabel(\"Number of observations\")\n",
    "name = 'offset_hess1_hd.pdf'\n",
    "plt.savefig(patho/name, overwrite=True)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(offset_hess1u, 40, (0.0, 2.1))\n",
    "    \n",
    "plt.title(\"Total HESS 1U\")\n",
    "plt.grid()\n",
    "plt.xlabel(\"Offset with respect to the central source(deg)\")\n",
    "plt.ylabel(\"Number of observations\")\n",
    "name = 'offset_hess1u_hd.pdf'\n",
    "plt.savefig(patho/name, overwrite=True)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(offset_hess2, 40, (0.0, 2.1))\n",
    "    \n",
    "plt.title(\"Total HESS 2\")\n",
    "plt.grid()\n",
    "plt.xlabel(\"Offset with respect to the central source(deg)\")\n",
    "plt.ylabel(\"Number of observations\")\n",
    "name = 'offset_hess2_hd.pdf'\n",
    "plt.savefig(patho/name, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zenith angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracing zenithal angle of observation for each year\n",
    "\n",
    "histo_zen = dict()\n",
    "\n",
    "pathz = Path(path/\"angzen\")\n",
    "pathz.mkdir(exist_ok=True)\n",
    "\n",
    "for year in range(2004,2020):  \n",
    "    histo_zen[year] = []\n",
    "    \n",
    "    obs_table_year = obs_table.select_observations(year_intervals[year])\n",
    "    histo_zen[year] = obs_table_year[\"ZEN_PNT\"].tolist()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.hist(histo_zen[year], 40, (5.0, 65.0))\n",
    "    \n",
    "    plt.title(str(year))\n",
    "    plt.grid()\n",
    "    plt.xlabel(\"Zenithal angle (deg)\")\n",
    "    plt.ylabel(\"Number of observations\")\n",
    "    \n",
    "    name = \"angzen_\"+str(year)+\"fr.pdf\"\n",
    "    plt.savefig(pathz/name, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zen_tot = obs_table[\"ZEN_PNT\"].tolist()\n",
    "avg = np.mean(zen_tot)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(zen_tot, 40, (5.0, 65.0))\n",
    "    \n",
    "plt.title('Total (2004-2019), mean = {0:0.2f}°'.format(avg))\n",
    "plt.grid()\n",
    "plt.xlabel(\"Zenith angle (deg)\")\n",
    "plt.ylabel(\"Number of observations\")\n",
    "\n",
    " \n",
    "name = \"angzen_tot_fr.pdf\"\n",
    "plt.savefig(pathz/name, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding up LiveTime for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "\n",
    "for year in range(2004,2020):\n",
    "    obs_table_year = obs_table.select_observations(year_intervals[year])\n",
    "    res.append(sum(obs_table_year[\"LIVETIME\"].tolist())/3600)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.plot(range(2004,2020), res)\n",
    "plt.grid()\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Total LIVETIME (hours)\")\n",
    "    \n",
    "name = path/\"livetime_plot_fr.pdf\"\n",
    "plt.savefig(name, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# affichage des histogrammes des énergies de seuil obtenu par bkg-peak, année par année\n",
    "\n",
    "for year in range(2004,2020):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.hist(histo_thresh[year], 20)\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.xlabel(\"Threshold energies (TeV)\")\n",
    "    plt.ylabel(\"Number of observations\")\n",
    "    \n",
    "    name = \"ethresh_\"+str(year)+\".pdf\"\n",
    "    \n",
    "    #plt.savefig(path/name, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
