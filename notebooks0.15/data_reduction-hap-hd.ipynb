{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolution de l'émission du centre galactique et de l'émission diffuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.15'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "import astropy.units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.convolution import Tophat2DKernel\n",
    "from regions import CircleSkyRegion, RectangleSkyRegion\n",
    "\n",
    "from gammapy.detect import compute_lima_on_off_image\n",
    "from gammapy.data import DataStore\n",
    "from gammapy.irf import make_mean_psf\n",
    "from gammapy.maps import Map, MapAxis, WcsGeom\n",
    "from gammapy.cube import (\n",
    "    MapDatasetMaker,\n",
    "    PSFKernel,\n",
    "    MapDataset,\n",
    "    RingBackgroundMaker,\n",
    "    SafeMaskMaker,\n",
    "    #RingBackgroundEstimator,\n",
    ")\n",
    "from gammapy.modeling.models import (\n",
    "    SkyModel,\n",
    "    BackgroundModel,\n",
    "    PowerLawSpectralModel,\n",
    "    PowerLaw2SpectralModel,\n",
    "    PointSpatialModel,\n",
    "    ExpCutoffPowerLawSpectralModel,\n",
    "    SkyDiffuseCube,\n",
    "    TemplateSpatialModel\n",
    ")\n",
    "from gammapy.modeling import Fit\n",
    "from astropy.time import Time\n",
    "\n",
    "src_pos = SkyCoord(359.94, -0.04, unit=\"deg\", frame=\"galactic\")\n",
    "\n",
    "import gammapy\n",
    "gammapy.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emprunt d'une classe implémentée en 0.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "##FoV background estimation\n",
    "import logging\n",
    "from gammapy.maps import Map\n",
    "from gammapy.modeling import Fit, Datasets\n",
    "\n",
    "\n",
    "class FoVBackgroundMaker:\n",
    "    \"\"\"Normalize template background on the whole field-of-view.\n",
    "\n",
    "    The dataset background model can be simply scaled (method=\"scale\") or fitted (method=\"fit\")\n",
    "    on the dataset counts.\n",
    "\n",
    "    The normalization is performed outside the exclusion mask that is passed on init.\n",
    "\n",
    "    If a SkyModel is set on the input dataset and method is 'fit', its are frozen during\n",
    "    the fov normalization fit.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    method : str in ['fit', 'scale']\n",
    "        the normalization method to be applied. Default 'scale'.\n",
    "    exclusion_mask : `~gammapy.maps.WcsNDMap`\n",
    "        Exclusion mask\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, method=\"scale\", exclusion_mask=None):\n",
    "        if method in [\"fit\", \"scale\"]:\n",
    "            self.method = method\n",
    "        else:\n",
    "            raise ValueError(f\"Incorrect method for FoVBackgroundMaker: {method}.\")\n",
    "        self.exclusion_mask = exclusion_mask\n",
    "\n",
    "\n",
    "    def run(self, dataset):\n",
    "        \"\"\"Run FoV background maker.\n",
    "\n",
    "        Fit the background model norm\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : `~gammapy.cube.fit.MapDataset`\n",
    "            Input map dataset.\n",
    "\n",
    "        \"\"\"\n",
    "        mask_fit = dataset.mask_fit\n",
    "        dataset.mask_fit = self._reproject_exclusion_mask(dataset)\n",
    "\n",
    "        if self.method is \"fit\":\n",
    "            self._fit_bkg(dataset)\n",
    "        else:\n",
    "            self._scale_bkg(dataset)\n",
    "\n",
    "        dataset.mask_fit = mask_fit\n",
    "        return dataset\n",
    "\n",
    "\n",
    "    def _reproject_exclusion_mask(self, dataset):\n",
    "        \"\"\"Reproject the exclusion on the dataset geometry\"\"\"\n",
    "        mask_map = Map.from_geom(dataset.counts.geom)\n",
    "        if self.exclusion_mask is not None:\n",
    "            coords = dataset.counts.geom.get_coord()\n",
    "            vals = self.exclusion_mask.get_by_coord(coords)\n",
    "            mask_map.data += vals\n",
    "\n",
    "        return mask_map.data.astype(\"bool\")\n",
    "\n",
    "    def _fit_bkg(self, dataset):\n",
    "        \"\"\"Fit the FoV background model on the dataset counts data\"\"\"\n",
    "\n",
    "        # freeze all model components not related to background model\n",
    "        datasets = Datasets([dataset])\n",
    "\n",
    "        parameters_frozen = []\n",
    "        for par in datasets.parameters:\n",
    "            parameters_frozen.append(par.frozen)\n",
    "            if par not in dataset.background_model.parameters:\n",
    "                par.frozen = True\n",
    "\n",
    "        #!!!AL: relax titlt : BE CARREFULL !!!\n",
    "        dataset.background_model.tilt.frozen=False\n",
    "        \n",
    "        fit = Fit(datasets)\n",
    "        fit_result = fit.run()\n",
    "        if fit_result.success is False:\n",
    "            print(\"FoVBackgroundMaker failed. No fit convergence\")\n",
    "            \n",
    "\n",
    "        # Unfreeze parameters\n",
    "        for i, par in enumerate(datasets.parameters):\n",
    "            par.frozen = parameters_frozen[i]\n",
    "\n",
    "    def _scale_bkg(self, dataset):\n",
    "        \"\"\"Fit the FoV background model on the dataset counts data\"\"\"\n",
    "        mask = dataset.mask\n",
    "        count_tot = dataset.counts.data[mask].sum()\n",
    "        bkg_tot = dataset.background_model.map.data[mask].sum()\n",
    "\n",
    "        if count_tot <= 0.0:\n",
    "            print(\"FoVBackgroundMaker failed. No counts found outside exclusion mask\")\n",
    "        elif bkg_tot <= 0.0:\n",
    "            print(\"FoVBackgroundMaker failed. No positive background found outside exclusion mask\")\n",
    "        else:\n",
    "            scale = count_tot / bkg_tot\n",
    "            dataset.background_model.norm.value = scale\n",
    "            #print(\"bkg scale = \",scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fabrication des mapdatasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define which data to use and print some information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_store_hess1 = DataStore.from_dir(\"$GAMMAPY_DATA/hap-hd_Prod05/hess1/std_ImPACT_fullEnclosure\")\n",
    "data_store_hess1u = DataStore.from_dir(\"$GAMMAPY_DATA/hap-hd_Prod05/hess1u/std_ImPACT_fullEnclosure\")\n",
    "data_store_hess2 = DataStore.from_dir(\"$GAMMAPY_DATA/hap-hd_Prod05/hess2/std_ImPACT_fullEnclosure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.coordinates import Angle\n",
    "\n",
    "\n",
    "selection = dict(type='sky_circle', frame='galactic',\n",
    "                 lon=Angle(0, 'deg'),\n",
    "                 lat=Angle(0, 'deg'),\n",
    "                 radius=Angle(2, 'deg'),\n",
    "                 border=Angle(0, 'deg'))\n",
    "\n",
    "obs_table1 = data_store_hess1.obs_table.select_observations(selection)\n",
    "obs_table1u = data_store_hess1u.obs_table.select_observations(selection)\n",
    "obs_table2 = data_store_hess2.obs_table.select_observations(selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Time object: scale='tt' format='mjd' value=[56452.84258315 56452.86352065 56452.884435   56452.90534935\n",
       " 56452.92637944 56452.94730537 56452.96833546 56452.97964333\n",
       " 56453.00066185 56456.88909935 56456.91826602 56456.93928454\n",
       " 56456.96350907 56456.98452759 56457.00554611 56457.02656463\n",
       " 56457.04075444 56471.87213176 56472.87281463 56473.87053454\n",
       " 56473.89155306 56473.91258315 56473.93360167 56473.95462019\n",
       " 56474.85113639 56474.89372898 56474.93651833 56474.97874056\n",
       " 56476.85180769 56478.92235167 56478.96540722 56479.85420352\n",
       " 56479.8847475  56479.90576602 56479.92540722 56479.94642574\n",
       " 56479.96735167 56479.98837019 56480.0093887  56480.03040722\n",
       " 56481.85418037 56481.90301139 56481.9169813  56481.93929611\n",
       " 56481.96031463 56482.83173824 56482.88059241 56485.92038407\n",
       " 56508.89847435 56509.81369426 56509.90893731 56509.92986324\n",
       " 56510.78109009 56510.84712019 56510.88924981 56510.93243269\n",
       " 56511.80146046 56511.82238639 56511.84353222 56511.8607313\n",
       " 56511.891935   56511.91593963 56512.80900676 56512.87299981\n",
       " 56512.9006387  56512.92280306 56513.85158778 56513.8759512\n",
       " 56514.82747898 56514.85449287 56529.74952759 56529.7759975\n",
       " 56529.80384472 56529.82477065 56535.75071972 56535.77173824\n",
       " 56535.79267574 56535.81360167 56537.78679611 56537.82958546\n",
       " 56538.75176139 56538.77297667 56538.79400676 56538.81512944\n",
       " 56538.83614796 56538.85725907 56562.78799981 56562.81332389\n",
       " 56563.7685438  56563.81179611 56564.75769889 56566.77732852\n",
       " 56568.76006    56568.78136787 56569.76131    56569.78262944\n",
       " 56569.79777991 56569.82215491 56571.76886787 56721.14058083\n",
       " 56744.05590491 56748.05192343 56748.07333546 56748.0947475\n",
       " 56749.04103222 56749.06244426 56749.12742111 56749.14874056\n",
       " 56750.04392574 56750.06523361 56750.08655306 56750.10786093\n",
       " 56750.1291688  56752.05305769 56752.07436556 56752.095685\n",
       " 56752.11699287 56752.13830074 56776.05114796 56776.07246741\n",
       " 56776.09299981 56776.11430769 56776.13561556 56777.07889102\n",
       " 56777.10021046 56777.12151833 56778.09360167 56778.13631\n",
       " 56779.02085861 56779.04178454 56779.08441185 56779.10581231\n",
       " 56779.12769889 56780.10754843 56781.07509472 56781.11772204\n",
       " 56782.02348593 56782.04490954 56782.06621741 56784.99837019\n",
       " 56785.01967806 56785.05911093 56787.0807313  56802.99048824\n",
       " 56803.01171509 56805.95999056 56805.98102065 56806.00234009\n",
       " 56808.93337019 56808.95458546 56808.97581231 56810.0372938\n",
       " 56810.05842806 56810.08646046 56810.10739796 56810.12833546\n",
       " 56810.1481387  56811.01852065 56811.03973593 56811.0609512\n",
       " 56811.08217806 56811.11034935 56811.1315762  56811.15332389\n",
       " 56826.90102065 56829.90133315 56829.9238563  56829.94489796\n",
       " 56832.02745583 56832.04830074 56832.06915722 56832.9401063\n",
       " 56834.02841648 56834.04972435 56834.07103222 56835.00122898\n",
       " 56835.01752528 56835.03846278 56835.87728222 56835.89859009\n",
       " 56835.91971278 56835.98620583 56836.00548824 56836.0266225\n",
       " 56836.04867111 56836.0697938  56838.03593963 56838.05715491\n",
       " 56838.85206231 56838.87328917 56838.89431926 56838.92137944\n",
       " 56838.96275676 56838.98378685 56839.00464333 56839.02585861\n",
       " 56839.98641417 56840.00773361 56840.02894889 56840.05026833\n",
       " 56840.96879843 56841.96850907 56842.03171509 56842.04359009\n",
       " 56842.98163407 56843.01119426 56843.03203917 56844.01641417\n",
       " 56844.03764102 56845.01156463 56855.89956231 56856.86908778\n",
       " 56856.88994426 56856.91589333 56857.86678454 56857.88781463\n",
       " 56857.90865954 56858.85369426 56858.87490954 56858.89612481\n",
       " 56858.92676139 56859.86316185 56859.88444657 56859.90566185\n",
       " 56859.9269813  56860.8190762  56860.84021046 56860.86143731\n",
       " 56860.8822938  56860.9031387  56860.92426139 56861.8507313\n",
       " 56861.89317343 56862.86422667 56862.88161093 56863.85139102\n",
       " 56863.8722475  56863.89309241 56863.91430769 56864.85997898\n",
       " 56864.8740762  56864.88806926 56864.91112481 56865.81488639\n",
       " 56865.83610167 56865.85740954 56865.87863639 56865.89897204\n",
       " 56866.83738639 56866.86155306 56866.9040762  56867.86201602\n",
       " 56867.8832313  56867.8993887  56868.88450444 57130.02908778\n",
       " 57130.04982852 57130.07053454 57130.09195815 57130.11647204\n",
       " 57131.01546509 57131.08217806 57131.10288407 57131.12364796\n",
       " 57131.98100907 57132.00171509 57132.02246741 57132.04332389\n",
       " 57132.06405306 57132.08477065 57132.09717806 57133.00707389\n",
       " 57133.02774519 57133.04840491 57133.0691225  57133.08985167\n",
       " 57133.11051139 57133.13119426 57133.99463176 57134.01531463\n",
       " 57134.0359975  57134.04985167 57134.07070815 57134.0913563\n",
       " 57134.11212019 57134.9997938  57135.02034935 57135.04089333\n",
       " 57135.06143731 57135.08213176 57135.10284935 57136.00144889\n",
       " 57136.02027991 57136.04414565 57136.07039565 57136.09168037\n",
       " 57137.99369426 57138.00173824 57138.01490954 57138.02381\n",
       " 57139.02967806 57140.10165722 57140.11569657 57141.07592806\n",
       " 57141.09665722 57152.87256    57152.89663407 57152.91562713\n",
       " 57153.86386787 57154.95960861 57157.05815028 57159.05361324\n",
       " 57161.05196972 57161.99151833 57162.01538407 57162.02952759\n",
       " 57162.07134472 57164.04759472 57165.06409935 57166.03930769\n",
       " 57166.0597012  57166.08036093 57166.98959704 57167.01040722\n",
       " 57167.03117111 57167.05159935 57167.07203917 57182.95038407\n",
       " 57183.93578917 57183.95610167 57183.97701602 57211.84271046\n",
       " 57211.86302296 57211.88371741 57211.90445815 57211.9247475\n",
       " 57212.85634472 57212.8766688  57212.89730537 57212.9176063\n",
       " 57212.93790722 57212.95825444 57213.77107852 57213.79137944\n",
       " 57213.8116688  57213.83196972 57213.87828917 57213.8991688\n",
       " 57213.9194813  57213.96008315 57213.98039565 57214.85809241\n",
       " 57214.87869426 57214.89937713 57214.9197012  57214.94000213\n",
       " 57214.96030306 57214.98083546 57215.76891417 57215.78919194\n",
       " 57215.80949287 57215.82341648 57215.84371741 57215.86405306\n",
       " 57215.88457389 57215.90487481 57215.92519889 57215.94548824\n",
       " 57215.96578917 57215.98610167 57216.7612637  57216.78156463\n",
       " 57216.80184241 57216.82213176 57216.84243269 57216.86274519\n",
       " 57216.88328917 57216.93473593 57236.74464333 57236.76514102\n",
       " 57237.79273361 57238.74507157 57272.82654148 57484.11676139\n",
       " 57484.12707389 57485.09815028 57486.06761787 57487.07404148\n",
       " 57487.09477065 57488.06297667 57488.08350907 57488.1028262\n",
       " 57488.12316185 57489.07200444 57489.09230537 57489.11259472\n",
       " 57490.06504843 57490.08588176 57492.05367111 57492.08173824\n",
       " 57492.10216648 57492.12250213 57493.04257157 57493.06293037\n",
       " 57493.08326602 57493.10357852 57493.13553454 57495.09589333\n",
       " 57496.11155306 57496.13184241 57497.11913407 57511.04696972\n",
       " 57512.02152991 57512.08622898 57513.01625213 57513.07764102\n",
       " 57513.09794194 57519.05681926 57522.02337019 57538.9937637\n",
       " 57539.99396046 57540.02457389 57540.99506    57542.9878725\n",
       " 57543.00950444 57543.02980537 57543.98685398 57544.00714333\n",
       " 57544.99634472 57545.0166688  57547.01953917 57547.9597012\n",
       " 57548.94861324 57548.96892574 57549.93440028 57549.99614796\n",
       " 57569.82392574 57569.85507157 57569.87583546 57570.82904148\n",
       " 57570.95342806 57572.82453917 57597.78460861 57597.85952759\n",
       " 57605.81530306 57606.83117111 57620.76985167 57621.7697012\n",
       " 57621.79002528]>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_table2.time_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2004  = dict(type='time_box', time_range= Time(['2004-01-01T00:00:00', '2004-12-31T23:59:59']))\n",
    "t2005  = dict(type='time_box', time_range= Time(['2005-01-01T00:00:00', '2005-12-31T23:59:59']))\n",
    "t2006  = dict(type='time_box', time_range= Time(['2006-01-01T00:00:00', '2006-12-31T23:59:59']))\n",
    "t2007  = dict(type='time_box', time_range= Time(['2007-01-01T00:00:00', '2007-12-31T23:59:59']))\n",
    "t2008  = dict(type='time_box', time_range= Time(['2008-01-01T00:00:00', '2008-12-31T23:59:59']))\n",
    "t2009  = dict(type='time_box', time_range= Time(['2009-01-01T00:00:00', '2009-12-31T23:59:59']))\n",
    "t2010  = dict(type='time_box', time_range= Time(['2010-01-01T00:00:00', '2010-12-31T23:59:59']))\n",
    "t2011  = dict(type='time_box', time_range= Time(['2011-01-01T00:00:00', '2011-12-31T23:59:59']))\n",
    "t2012  = dict(type='time_box', time_range= Time(['2012-01-01T00:00:00', '2012-12-31T23:59:59']))\n",
    "t2013  = dict(type='time_box', time_range= Time(['2013-01-01T00:00:00', '2013-12-31T23:59:59']))\n",
    "t2014  = dict(type='time_box', time_range= Time(['2014-01-01T00:00:00', '2014-12-31T23:59:59']))\n",
    "t2015  = dict(type='time_box', time_range= Time(['2015-01-01T00:00:00', '2015-12-31T23:59:59']))\n",
    "t2016  = dict(type='time_box', time_range= Time(['2016-01-01T00:00:00', '2016-12-31T23:59:59']))\n",
    "t2017  = dict(type='time_box', time_range= Time(['2017-01-01T00:00:00', '2017-12-31T23:59:59']))\n",
    "t2018  = dict(type='time_box', time_range= Time(['2018-01-01T00:00:00', '2018-12-31T23:59:59']))\n",
    "t2019  = dict(type='time_box', time_range= Time(['2019-01-01T00:00:00', '2019-12-31T23:59:59']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sélection par année et tri des observations (on retire celles qui n'ont pas toutes les IRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation retirée : 20191\n",
      "Observation retirée : 27162\n",
      "Observation retirée : 27204\n",
      "Observation retirée : 27329\n",
      "Observation retirée : 27578\n",
      "Observation retirée : 27607\n",
      "Observation retirée : 27638\n"
     ]
    }
   ],
   "source": [
    "year_intervals = { 2004 : t2004, 2005 : t2005, 2006 : t2006, 2007 : t2007,\n",
    "                      2008 : t2008, 2009 : t2009, 2010 : t2010, 2011 : t2011,\n",
    "                      2012 : t2012, 2013 : t2013, 2014 : t2014, 2015 : t2015,\n",
    "                      2016 : t2016, 2017 : t2017, 2018 : t2018, 2019 : t2019}\n",
    "\n",
    "yearly_obs1 = dict()\n",
    "\n",
    "for year in range(2004,2020) :\n",
    "    \n",
    "    obs_table_year = obs_table1.select_observations(year_intervals[year])\n",
    "    ids = obs_table_year[\"OBS_ID\"].tolist()\n",
    "    observations_year = data_store_hess1.get_observations(ids, skip_missing=True)\n",
    "    \n",
    "    for obs in observations_year:\n",
    "        try:\n",
    "            obs.aeff\n",
    "            obs.edisp\n",
    "            obs.psf\n",
    "            obs.bkg\n",
    "        except:\n",
    "            ids.remove(obs.obs_id)\n",
    "            print(\"Observation retirée : \" + str(obs.obs_id))\n",
    "            \n",
    "    observations_year = data_store_hess1.get_observations(ids, skip_missing=True)\n",
    "    yearly_obs1[year] = observations_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation retirée : 146924\n",
      "Observation retirée : 146965\n",
      "Observation retirée : 147004\n",
      "Observation retirée : 147007\n",
      "Observation retirée : 147008\n",
      "Observation retirée : 147009\n",
      "Observation retirée : 147010\n",
      "Observation retirée : 147011\n",
      "Observation retirée : 147012\n",
      "Observation retirée : 147055\n",
      "Observation retirée : 147056\n",
      "Observation retirée : 147077\n",
      "Observation retirée : 147121\n",
      "Observation retirée : 147124\n",
      "Observation retirée : 153153\n"
     ]
    }
   ],
   "source": [
    "yearly_obs1u = dict()\n",
    "\n",
    "for year in range(2004,2020) :\n",
    "    \n",
    "    obs_table_year = obs_table1u.select_observations(year_intervals[year])\n",
    "    ids = obs_table_year[\"OBS_ID\"].tolist()\n",
    "    observations_year = data_store_hess1u.get_observations(ids, skip_missing=True)\n",
    "    \n",
    "    for obs in observations_year:\n",
    "        try:\n",
    "            obs.aeff\n",
    "            obs.edisp\n",
    "            obs.psf\n",
    "            obs.bkg\n",
    "        except:\n",
    "            ids.remove(obs.obs_id)\n",
    "            print(\"Observation retirée : \" + str(obs.obs_id))\n",
    "            \n",
    "    observations_year = data_store_hess1u.get_observations(ids, skip_missing=True)\n",
    "    yearly_obs1u[year] = observations_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_obs2 = dict()\n",
    "\n",
    "for year in range(2004,2020) :\n",
    "    \n",
    "    obs_table_year = obs_table2.select_observations(year_intervals[year])\n",
    "    ids = obs_table_year[\"OBS_ID\"].tolist()\n",
    "    observations_year = data_store_hess2.get_observations(ids, skip_missing=True)\n",
    "    \n",
    "    for obs in observations_year:\n",
    "        try:\n",
    "            obs.aeff\n",
    "            obs.edisp\n",
    "            obs.psf\n",
    "            obs.bkg\n",
    "        except:\n",
    "            ids.remove(obs.obs_id)\n",
    "            print(\"Observation retirée : \" + str(obs.obs_id))\n",
    "            \n",
    "    observations_year = data_store_hess2.get_observations(ids, skip_missing=True)\n",
    "    yearly_obs2[year] = observations_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<i>ObservationTable length=0</i>\n",
       "<table id=\"table140701328041688\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>ALT_PNT</th><th>AZ_PNT</th><th>DATE-END</th><th>DATE-OBS</th><th>DEADC</th><th>DEC_OBJ</th><th>DEC_PNT</th><th>EVENT_COUNT</th><th>EVENT_DEC_MEDIAN</th><th>EVENT_ENERGY_MEDIAN</th><th>EVENT_RA_MEDIAN</th><th>EVENT_TIME_MAX</th><th>EVENT_TIME_MIN</th><th>GLAT_PNT</th><th>GLON_PNT</th><th>LIVETIME</th><th>MUONCORR</th><th>MUONEFF</th><th>N_TELS</th><th>OBJECT</th><th>OBS_ID</th><th>ONTIME</th><th>QUALITY</th><th>RA_OBJ</th><th>RA_PNT</th><th>TELLIST</th><th>TIME-END</th><th>TIME-OBS</th><th>TSTART</th><th>TSTOP</th><th>ZEN_PNT</th><th>BKG_SCALE</th></tr></thead>\n",
       "<thead><tr><th>float32</th><th>float32</th><th>bytes10</th><th>bytes10</th><th>float32</th><th>float32</th><th>float32</th><th>int64</th><th>float32</th><th>float32</th><th>float32</th><th>float64</th><th>float64</th><th>float32</th><th>float32</th><th>float32</th><th>float32</th><th>float32</th><th>int64</th><th>bytes27</th><th>int64</th><th>float32</th><th>int64</th><th>float32</th><th>float32</th><th>bytes7</th><th>bytes12</th><th>bytes12</th><th>float64</th><th>float64</th><th>float32</th><th>float32</th></tr></thead>\n",
       "</table>"
      ],
      "text/plain": [
       "<ObservationTable length=0>\n",
       "ALT_PNT  AZ_PNT DATE-END DATE-OBS  DEADC  ...  TSTART  TSTOP  ZEN_PNT BKG_SCALE\n",
       "float32 float32 bytes10  bytes10  float32 ... float64 float64 float32  float32 \n",
       "------- ------- -------- -------- ------- ... ------- ------- ------- ---------"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_table1.select_observations(year_intervals[2004])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création de la géométrie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "emin, emax = [0.5, 100] * u.TeV\n",
    "\n",
    "energy_axis = MapAxis.from_bounds(\n",
    "    emin.value, emax.value, 20, unit=\"TeV\", name=\"energy\", interp=\"log\"\n",
    ")\n",
    "geom = WcsGeom.create(\n",
    "    skydir=(0, 0),\n",
    "    binsz=0.02,\n",
    "    width=(10, 8),\n",
    "    coordsys=\"GAL\",\n",
    "    proj=\"CAR\",\n",
    "    axes=[energy_axis],\n",
    ")\n",
    "\n",
    "energy_axis_true = MapAxis.from_bounds(\n",
    "    0.3, 200, 30, unit=\"TeV\", name=\"energy\", interp=\"log\"\n",
    ")\n",
    "\n",
    "# peut être passer à plus que 20 bins en énergie vraie ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapdataset_dict = {}\n",
    "\n",
    "for k in range (2004,2020):\n",
    "    name = \"map\" + str(k)\n",
    "    mapdataset_dict[k] = MapDataset.create(\n",
    "    geom=geom, energy_axis_true=energy_axis_true, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A noter que nos datasets ont un axe d'énergie vraie, pas biné très finement cela dit..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fabrication des mapdatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'yearly_obs1u' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'yearly_obs1u' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "exclusion_region = RectangleSkyRegion(src_pos, 3*u.deg, 1*u.deg)\n",
    "exclusion_mask = geom.region_mask([exclusion_region], inside=False)\n",
    "exclusion_mask = Map.from_geom(geom, data=exclusion_mask)\n",
    "\n",
    "for year in range(2004,2020):\n",
    "    \n",
    "    offset_max = 2.0 * u.deg\n",
    "    maker = MapDatasetMaker()\n",
    "    maker_safe_mask = SafeMaskMaker(methods=[\"offset-max\", \"bkg-peak\"], offset_max=offset_max)\n",
    "    maker_bkg = FoVBackgroundMaker(\"scale\", exclusion_mask)\n",
    "    \n",
    "    # au lieu de \"scale\" prendre \"fit\" en donnant au dataset un masque sur les bin en énergie (prendre à partir de 1 TeV par ex, même si c'est déjà haut 500 GeV)\n",
    "    # ou alors on met \"bkg-peak\" pour le safemaskmaker (le premier bin en énergie est retiré)\n",
    "    \n",
    "    # refaire des mapdataset en excluant le premier bin pour voir si ça change\n",
    "    \n",
    "    spectrum = PowerLaw2SpectralModel(index=2.3)\n",
    "\n",
    "    for obs in yearly_obs1[year]:\n",
    "        # First a cutout of the target map is produced\n",
    "        cutout = mapdataset_dict[year].cutout(obs.pointing_radec, width=2 * offset_max)\n",
    "\n",
    "        # A MapDataset is filled in this cutout geometry\n",
    "        dataset = maker.run(cutout, obs)\n",
    "        \n",
    "        # The data quality cut is applied\n",
    "        dataset = maker_safe_mask.run(dataset, obs)\n",
    "        \n",
    "        # stocker la distributions des seuils par années (utiliser np.where(mask.data[:, 250,250]))\n",
    "        \n",
    "        dataset = maker_bkg.run(dataset)\n",
    "        \n",
    "        # The resulting dataset cutout is stacked onto the final one\n",
    "        mapdataset_dict[year].stack(dataset)\n",
    "        \n",
    "    for obs in yearly_obs1u[year]:\n",
    "        # First a cutout of the target map is produced\n",
    "        cutout = mapdataset_dict[year].cutout(obs.pointing_radec, width=2 * offset_max)\n",
    "\n",
    "        # A MapDataset is filled in this cutout geometry\n",
    "        dataset = maker.run(cutout, obs)\n",
    "        \n",
    "        # The data quality cut is applied\n",
    "        dataset = maker_safe_mask.run(dataset, obs)\n",
    "        \n",
    "        # stocker la distributions des seuils par années (utiliser np.where(mask.data[:, 250,250]))\n",
    "        \n",
    "        dataset = maker_bkg.run(dataset)\n",
    "        \n",
    "        # The resulting dataset cutout is stacked onto the final one\n",
    "        mapdataset_dict[year].stack(dataset)\n",
    "        \n",
    "    for obs in yearly_obs2[year]:\n",
    "        # First a cutout of the target map is produced\n",
    "        cutout = mapdataset_dict[year].cutout(obs.pointing_radec, width=2 * offset_max)\n",
    "\n",
    "        # A MapDataset is filled in this cutout geometry\n",
    "        dataset = maker.run(cutout, obs)\n",
    "        \n",
    "        # The data quality cut is applied\n",
    "        dataset = maker_safe_mask.run(dataset, obs)\n",
    "        \n",
    "        # stocker la distributions des seuils par années (utiliser np.where(mask.data[:, 250,250]))\n",
    "        \n",
    "        dataset = maker_bkg.run(dataset)\n",
    "        \n",
    "        # The resulting dataset cutout is stacked onto the final one\n",
    "        mapdataset_dict[year].stack(dataset)\n",
    "        \n",
    "    #mask = dataset.mask_safe.copy()\n",
    "    #mask.data[0:3,:,:] = False\n",
    "    #dataset.mask_fit = mask\n",
    "    \n",
    "    # essayer de se placer systématiquement au dessus du seuil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "safemask = mapdataset_dict[2004].mask_safe\n",
    "expo = mapdataset_dict[2004].exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce = safemask.reduce_over_axes(func=np.logical_or)\n",
    "safemask.data[:, 250 ,250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lors du stack on n'est pas surs de comment les différents seuils sont pris en compte.\n",
    "Regardez les différentes énergies seuils, voir la distribution et enlever ceux qui sont trop haut. Ou bien juste partir beaucoup plus haut que le seuil donné par bkg-peak.\n",
    "\n",
    "Eventuellement participer à l'implémentation de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Pour information, la fonction associée à 'bkg-peak', rien à changer a priori, c'est simple et clair. ça fait ce que c'est censé faire, \n",
    "# à la limite on peut penser à d'autres méthodes pour couper en énergie\n",
    "\n",
    "def make_mask_energy_bkg_peak(dataset):\n",
    "        \"\"\"Make safe energy mask based on the binned background.\n",
    "\n",
    "        The energy threshold is defined as the upper edge of the energy\n",
    "        bin with the highest predicted background rate. This method is motivated\n",
    "        by its use in the HESS DL3 validation paper: https://arxiv.org/pdf/1910.08088.pdf\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : `~gammapy.modeling.Dataset`\n",
    "            Dataset to compute mask for.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mask_safe : `~numpy.ndarray`\n",
    "            Safe data range mask.\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(dataset, (MapDataset, MapDatasetOnOff)):\n",
    "            background_spectrum = dataset.background_model.map.get_spectrum()\n",
    "            counts = dataset.counts.geom\n",
    "        else:\n",
    "            background_spectrum = dataset.background\n",
    "            counts = dataset.counts\n",
    "\n",
    "        idx = np.argmax(background_spectrum.data)         # trouve l'indice du max du spectre du bkg\n",
    "        e_min = background_spectrum.energy.edges[idx + 1] # identifie l'emin comme l'énergie suivante du max\n",
    "        return counts.energy_mask(emin=e_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Pour information, la méthode 'stack' de la classe MapDataset, on a un mapdataset (self) auquel on en rajoute un autre (other)\n",
    "\n",
    "def stack(self, other):\n",
    "        \"\"\"Stack another dataset in place.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        other: `~gammapy.cube.MapDataset`\n",
    "            Map dataset to be stacked with this one.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.counts and other.counts:\n",
    "            self.counts *= self.mask_safe\n",
    "            self.counts.stack(other.counts, weights=other.mask_safe)\n",
    "\n",
    "        if self.exposure and other.exposure:                                     # ICI\n",
    "            mask_image = self.mask_safe.reduce_over_axes(func=np.logical_or)\n",
    "            self.exposure *= mask_image.data\n",
    "            \n",
    "            # TODO: apply energy dependent mask to exposure. Does this require\n",
    "            #  a mask_safe in true energy?\n",
    "            \n",
    "            mask_image_other = other.mask_safe.reduce_over_axes(func=np.logical_or)\n",
    "            self.exposure.stack(other.exposure, weights=mask_image_other)\n",
    "            \n",
    "            # on applique reduce_over_axes aux deux safe_mask, donc on les réduit sur les axes non-spatiaux \n",
    "            # (mais on prend le meilleur masque si les énergies sont masquées différemment)\n",
    "            # donc on ne prend pas en compte le masquage éventuel en énergie lorsqu'on modifie l'exposure, \n",
    "            # dans : self.exposure *= mask_image.data (elle est multipliée d'un bloc)\n",
    "            \n",
    "            # derrière il semble avoir un souci entre énergie vraie et énergie recombinée\n",
    "            \n",
    "            # On finit avec la fonction stack de WcsNDMap, \n",
    "            # avec des weights qui RAJOUTENT le nouveau masque \n",
    "            # (s'il est plus grand, on supprime de nouveaux bins, s'il est plus petit, les bins effacés le restent), logique avec le concept de 'stack'\n",
    "            # la question est de savoir si appliquer un masque energie dépendant sur la carte d'exposition nécessite un mask safe en énergie vraie\n",
    "            # oui car l'exposition a des bins en énergie vraie et pas énergie recombinée\n",
    "            # mais il suffirait de \"faire la conversion\"\n",
    "            \n",
    "        if self.background_model and other.background_model:    \n",
    "            bkg = self.background_model.evaluate()\n",
    "            bkg *= self.mask_safe\n",
    "            other_bkg = other.background_model.evaluate()\n",
    "            bkg.stack(other_bkg, weights=other.mask_safe)\n",
    "\n",
    "            self.background_model = BackgroundModel(\n",
    "                bkg, name=self.background_model.name\n",
    "            )\n",
    "\n",
    "        if self.mask_safe is not None and other.mask_safe is not None:            # ICI en fait non ya rien à changer ici\n",
    "            self.mask_safe.stack(other.mask_safe)\n",
    "            \n",
    "            # juste une fonction 'stack' celle de WcsNDMap\n",
    "\n",
    "        if self.psf and other.psf:\n",
    "            if isinstance(self.psf, PSFMap) and isinstance(other.psf, PSFMap):\n",
    "                mask_irf = self._mask_safe_irf(self.psf.psf_map, mask_image)\n",
    "                self.psf.psf_map *= mask_irf.data\n",
    "                self.psf.exposure_map *= mask_irf.data\n",
    "\n",
    "                mask_image_other = other.mask_safe.reduce_over_axes(func=np.logical_or)\n",
    "                mask_irf_other = self._mask_safe_irf(\n",
    "                    other.psf.psf_map, mask_image_other\n",
    "                )\n",
    "                self.psf.stack(other.psf, weights=mask_irf_other)\n",
    "            else:\n",
    "                raise ValueError(\"Stacking of PSF kernels not supported\")\n",
    "\n",
    "        if self.edisp and other.edisp:\n",
    "            if isinstance(self.edisp, EDispMap) and isinstance(other.edisp, EDispMap):\n",
    "                mask_irf = self._mask_safe_irf(self.edisp.edisp_map, mask_image)\n",
    "                self.edisp.edisp_map *= mask_irf.data\n",
    "                self.edisp.exposure_map *= mask_irf.data\n",
    "\n",
    "                mask_image_other = other.mask_safe.reduce_over_axes(func=np.logical_or)\n",
    "                mask_irf_other = self._mask_safe_irf(\n",
    "                    other.edisp.edisp_map, mask_image_other\n",
    "                )\n",
    "                self.edisp.stack(other.edisp, weights=mask_irf_other)\n",
    "            else:\n",
    "                raise ValueError(\"Stacking of edisp kernels not supported\")\n",
    "\n",
    "        if self.gti and other.gti:\n",
    "            self.gti = self.gti.stack(other.gti).union()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Au sujet de l'énergie vraie : elle n'intervient que dans la fonction create où elle est donnée, et permet de créer les cubes pour l'exposition, la psf et l'edisp\n",
    "\n",
    "geom_image = geom.to_image() # on prend la geom 3d on a passe en 2d pour la repasser en 3d avec l'axe d'énergie vraie\n",
    "geom_exposure = geom_image.to_cube([energy_axis_true])\n",
    "geom_irf = geom_image.to_binsz(binsz=binsz_irf) # ça on y touche pas apparemment\n",
    "geom_psf = geom_irf.to_cube([rad_axis, energy_axis_true])\n",
    "geom_edisp = geom_irf.to_cube([migra_axis, energy_axis_true])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Conversion d'un masque en énergie recombinée en un masque en énergie vraie\n",
    "# sachant qu'a priori on veut pas masquer d'emblée toutes les énergies vraies en dessous du seuil en énergie recombinée\n",
    "# ça risque d'être lourd\n",
    "# on peut faire une execption si on a le même nombre de bins pour les deux\n",
    "\n",
    "def conv_mask_true(mask, erec, etrue):\n",
    "    # mask est une carte WcsNDMap de booléens\n",
    "    # erec et etrue sont des energy axis avec des edges et nbins a priori différents\n",
    "    \n",
    "    mask_true = mask.copy()\n",
    "    \n",
    "    # besoin d'une fonction qui pour un pixel regarde le dernièr bin en énergie rec masqué, et associe une liste de bin en énergie vraie masquée adéquatement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Pour info, méthode stack de la classe WcsNDMap\n",
    "\n",
    "def stack(self, other, weights=None):\n",
    "        \"\"\"Stack cutout into map.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        other : `WcsNDMap`\n",
    "            Other map to stack\n",
    "        weights : `WcsNDMap`\n",
    "            Array to be used as weights.\n",
    "        \"\"\"\n",
    "        if self.geom == other.geom:\n",
    "            parent_slices, cutout_slices = None, None\n",
    "        elif self.geom.is_aligned(other.geom):\n",
    "            slices = other.geom.cutout_info[\"parent-slices\"]\n",
    "            parent_slices = Ellipsis, slices[0], slices[1]\n",
    "\n",
    "            slices = other.geom.cutout_info[\"cutout-slices\"]\n",
    "            cutout_slices = Ellipsis, slices[0], slices[1]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Can only stack equivalent maps or cutout of the same map.\"\n",
    "            )\n",
    "\n",
    "        data = other.data[cutout_slices]\n",
    "\n",
    "        if weights is not None:\n",
    "            data = data * weights.data\n",
    "\n",
    "        self.data[parent_slices] += data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapdataset_dict[2006].mask_safe.data[:,250,250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.7          0.89710471   1.14970981   1.47344298   1.88833234\n",
      "   2.42004549   3.10147745   3.97478577   5.09399863   6.5283574\n",
      "   8.36660027  10.72245218  13.74166054  17.61101204  22.56988842\n",
      "  28.92507609  37.06974581  47.50777665  60.88492901  78.02879533\n",
      " 100.        ] TeV\n"
     ]
    }
   ],
   "source": [
    "print(energy_axis.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour masquer plus de bins que ce que fait le mask safe\n",
    "\n",
    "mask = dataset.mask_safe.copy()\n",
    "mask.data[0:3,:,:]=False\n",
    "dataset.mask_fit = mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegarde des mapdatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "#path = Path(\"../../../gamapy_data/mapdataset_hess/mapsdataset_hap-hd_zeta\")\n",
    "#path.mkdir(exist_ok=True)\n",
    "\n",
    "for year in range(2004,2020):\n",
    "    \n",
    "    filename = \"../../../gammapy_data/mapdataset_hess/mapsdataset_hap-hd_ImPACT/mapdataset\" +str(year)+\".fits.gz\"\n",
    "    mapdataset_dict[year].write(filename, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
