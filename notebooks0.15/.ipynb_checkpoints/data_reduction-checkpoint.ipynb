{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolution de l'émission du centre galactique et de l'émission diffuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.15'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "import astropy.units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.convolution import Tophat2DKernel\n",
    "from regions import CircleSkyRegion, RectangleSkyRegion\n",
    "\n",
    "from gammapy.detect import compute_lima_on_off_image\n",
    "from gammapy.data import DataStore\n",
    "from gammapy.irf import make_mean_psf\n",
    "from gammapy.maps import Map, MapAxis, WcsGeom\n",
    "from gammapy.cube import (\n",
    "    MapDatasetMaker,\n",
    "    PSFKernel,\n",
    "    MapDataset,\n",
    "    RingBackgroundMaker,\n",
    "    SafeMaskMaker,\n",
    "    #RingBackgroundEstimator,\n",
    ")\n",
    "from gammapy.modeling.models import (\n",
    "    SkyModel,\n",
    "    BackgroundModel,\n",
    "    PowerLawSpectralModel,\n",
    "    PowerLaw2SpectralModel,\n",
    "    PointSpatialModel,\n",
    "    ExpCutoffPowerLawSpectralModel,\n",
    "    SkyDiffuseCube,\n",
    "    TemplateSpatialModel\n",
    ")\n",
    "from gammapy.modeling import Fit\n",
    "from astropy.time import Time\n",
    "\n",
    "src_pos = SkyCoord(359.94, -0.04, unit=\"deg\", frame=\"galactic\")\n",
    "\n",
    "import gammapy\n",
    "gammapy.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emprunt d'une classe implémentée en 0.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "##FoV background estimation\n",
    "import logging\n",
    "from gammapy.maps import Map\n",
    "from gammapy.modeling import Fit, Datasets\n",
    "\n",
    "\n",
    "class FoVBackgroundMaker:\n",
    "    \"\"\"Normalize template background on the whole field-of-view.\n",
    "\n",
    "    The dataset background model can be simply scaled (method=\"scale\") or fitted (method=\"fit\")\n",
    "    on the dataset counts.\n",
    "\n",
    "    The normalization is performed outside the exclusion mask that is passed on init.\n",
    "\n",
    "    If a SkyModel is set on the input dataset and method is 'fit', its are frozen during\n",
    "    the fov normalization fit.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    method : str in ['fit', 'scale']\n",
    "        the normalization method to be applied. Default 'scale'.\n",
    "    exclusion_mask : `~gammapy.maps.WcsNDMap`\n",
    "        Exclusion mask\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, method=\"scale\", exclusion_mask=None):\n",
    "        if method in [\"fit\", \"scale\"]:\n",
    "            self.method = method\n",
    "        else:\n",
    "            raise ValueError(f\"Incorrect method for FoVBackgroundMaker: {method}.\")\n",
    "        self.exclusion_mask = exclusion_mask\n",
    "\n",
    "\n",
    "    def run(self, dataset):\n",
    "        \"\"\"Run FoV background maker.\n",
    "\n",
    "        Fit the background model norm\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : `~gammapy.cube.fit.MapDataset`\n",
    "            Input map dataset.\n",
    "\n",
    "        \"\"\"\n",
    "        mask_fit = dataset.mask_fit\n",
    "        dataset.mask_fit = self._reproject_exclusion_mask(dataset)\n",
    "\n",
    "        if self.method is \"fit\":\n",
    "            self._fit_bkg(dataset)\n",
    "        else:\n",
    "            self._scale_bkg(dataset)\n",
    "\n",
    "        dataset.mask_fit = mask_fit\n",
    "        return dataset\n",
    "\n",
    "\n",
    "    def _reproject_exclusion_mask(self, dataset):\n",
    "        \"\"\"Reproject the exclusion on the dataset geometry\"\"\"\n",
    "        mask_map = Map.from_geom(dataset.counts.geom)\n",
    "        if self.exclusion_mask is not None:\n",
    "            coords = dataset.counts.geom.get_coord()\n",
    "            vals = self.exclusion_mask.get_by_coord(coords)\n",
    "            mask_map.data += vals\n",
    "\n",
    "        return mask_map.data.astype(\"bool\")\n",
    "\n",
    "    def _fit_bkg(self, dataset):\n",
    "        \"\"\"Fit the FoV background model on the dataset counts data\"\"\"\n",
    "\n",
    "        # freeze all model components not related to background model\n",
    "        datasets = Datasets([dataset])\n",
    "\n",
    "        parameters_frozen = []\n",
    "        for par in datasets.parameters:\n",
    "            parameters_frozen.append(par.frozen)\n",
    "            if par not in dataset.background_model.parameters:\n",
    "                par.frozen = True\n",
    "\n",
    "        #!!!AL: relax titlt : BE CARREFULL !!!\n",
    "        dataset.background_model.tilt.frozen=False\n",
    "        \n",
    "        fit = Fit(datasets)\n",
    "        fit_result = fit.run()\n",
    "        if fit_result.success is False:\n",
    "            print(\"FoVBackgroundMaker failed. No fit convergence\")\n",
    "            \n",
    "\n",
    "        # Unfreeze parameters\n",
    "        for i, par in enumerate(datasets.parameters):\n",
    "            par.frozen = parameters_frozen[i]\n",
    "\n",
    "    def _scale_bkg(self, dataset):\n",
    "        \"\"\"Fit the FoV background model on the dataset counts data\"\"\"\n",
    "        mask = dataset.mask\n",
    "        count_tot = dataset.counts.data[mask].sum()\n",
    "        bkg_tot = dataset.background_model.map.data[mask].sum()\n",
    "\n",
    "        if count_tot <= 0.0:\n",
    "            print(\"FoVBackgroundMaker failed. No counts found outside exclusion mask\")\n",
    "        elif bkg_tot <= 0.0:\n",
    "            print(\"FoVBackgroundMaker failed. No positive background found outside exclusion mask\")\n",
    "        else:\n",
    "            scale = count_tot / bkg_tot\n",
    "            dataset.background_model.norm.value = scale\n",
    "            #print(\"bkg scale = \",scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fabrication des mapdatasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define which data to use and print some information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data store:\n",
      "HDU index table:\n",
      "BASE_DIR: /home/samuel/code/gammapy_data/ash_stereo_Prod17_Calib0834_thsq64\n",
      "Rows: 122853\n",
      "OBS_ID: 18092 -- 151486\n",
      "HDU_TYPE: ['aeff', 'bkg', 'edisp', 'events', 'gti', 'psf']\n",
      "HDU_CLASS: ['aeff_2d', 'bkg_2d', 'edisp_2d', 'events', 'gti', 'psf_3gauss', 'psf_table']\n",
      "\n",
      "\n",
      "Observation table:\n",
      "Observatory name: 'N/A'\n",
      "Number of observations: 20485\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_store = DataStore.from_dir(\"$GAMMAPY_DATA/ash_stereo_Prod17_Calib0834_thsq64\")\n",
    "data_store.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.coordinates import Angle\n",
    "\n",
    "\n",
    "selection = dict(type='sky_circle', frame='galactic',\n",
    "                 lon=Angle(0, 'deg'),\n",
    "                 lat=Angle(0, 'deg'),\n",
    "                 radius=Angle(2, 'deg'),\n",
    "                 border=Angle(0, 'deg'))\n",
    "\n",
    "obs_table = data_store.obs_table.select_observations(selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2004  = dict(type='time_box', time_range= Time(['2004-01-01T00:00:00', '2004-12-31T23:59:59']))\n",
    "t2005  = dict(type='time_box', time_range= Time(['2005-01-01T00:00:00', '2005-12-31T23:59:59']))\n",
    "t2006  = dict(type='time_box', time_range= Time(['2006-01-01T00:00:00', '2006-12-31T23:59:59']))\n",
    "t2007  = dict(type='time_box', time_range= Time(['2007-01-01T00:00:00', '2007-12-31T23:59:59']))\n",
    "t2008  = dict(type='time_box', time_range= Time(['2008-01-01T00:00:00', '2008-12-31T23:59:59']))\n",
    "t2009  = dict(type='time_box', time_range= Time(['2009-01-01T00:00:00', '2009-12-31T23:59:59']))\n",
    "t2010  = dict(type='time_box', time_range= Time(['2010-01-01T00:00:00', '2010-12-31T23:59:59']))\n",
    "t2011  = dict(type='time_box', time_range= Time(['2011-01-01T00:00:00', '2011-12-31T23:59:59']))\n",
    "t2012  = dict(type='time_box', time_range= Time(['2012-01-01T00:00:00', '2012-12-31T23:59:59']))\n",
    "t2013  = dict(type='time_box', time_range= Time(['2013-01-01T00:00:00', '2013-12-31T23:59:59']))\n",
    "t2014  = dict(type='time_box', time_range= Time(['2014-01-01T00:00:00', '2014-12-31T23:59:59']))\n",
    "t2015  = dict(type='time_box', time_range= Time(['2015-01-01T00:00:00', '2015-12-31T23:59:59']))\n",
    "t2016  = dict(type='time_box', time_range= Time(['2016-01-01T00:00:00', '2016-12-31T23:59:59']))\n",
    "t2017  = dict(type='time_box', time_range= Time(['2017-01-01T00:00:00', '2017-12-31T23:59:59']))\n",
    "t2018  = dict(type='time_box', time_range= Time(['2018-01-01T00:00:00', '2018-12-31T23:59:59']))\n",
    "t2019  = dict(type='time_box', time_range= Time(['2019-01-01T00:00:00', '2019-12-31T23:59:59']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sélection par année et tri des observations (on retire celles qui n'ont pas toutes les IRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found multiple HDU matching: OBS_ID = 20191, HDU_TYPE = psf, HDU_CLASS = None. Returning the first entry, which has HDU_TYPE = psf and HDU_CLASS = psf_3gauss\n",
      "Found multiple HDU matching: OBS_ID = 20193, HDU_TYPE = psf, HDU_CLASS = None. Returning the first entry, which has HDU_TYPE = psf and HDU_CLASS = psf_3gauss\n",
      "Found multiple HDU matching: OBS_ID = 20194, HDU_TYPE = psf, HDU_CLASS = None. Returning the first entry, which has HDU_TYPE = psf and HDU_CLASS = psf_3gauss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation retirée : 20191\n",
      "Observation retirée : 20193\n",
      "Observation retirée : 20194\n",
      "Observation retirée : 31539\n",
      "Observation retirée : 31577\n",
      "Observation retirée : 31578\n",
      "Observation retirée : 31579\n",
      "Observation retirée : 31580\n"
     ]
    }
   ],
   "source": [
    "year_intervals = { 2004 : t2004, 2005 : t2005, 2006 : t2006, 2007 : t2007,\n",
    "                      2008 : t2008, 2009 : t2009, 2010 : t2010, 2011 : t2011,\n",
    "                      2012 : t2012, 2013 : t2013, 2014 : t2014, 2015 : t2015,\n",
    "                      2016 : t2016, 2017 : t2017, 2018 : t2018, 2019 : t2019}\n",
    "\n",
    "yearly_obs = dict()\n",
    "\n",
    "for year in range(2004,2020) :\n",
    "    \n",
    "    obs_table_year = obs_table.select_observations(year_intervals[year])\n",
    "    ids = obs_table_year[\"OBS_ID\"].tolist()\n",
    "    observations_year = data_store.get_observations(ids, skip_missing=True)\n",
    "    \n",
    "    for obs in observations_year:\n",
    "        try:\n",
    "            obs.aeff\n",
    "            obs.edisp\n",
    "            obs.psf\n",
    "        except:\n",
    "            ids.remove(obs.obs_id)\n",
    "            print(\"Observation retirée : \" + str(obs.obs_id))\n",
    "            \n",
    "    observations_year = data_store.get_observations(ids, skip_missing=True)\n",
    "    yearly_obs[year] = observations_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création de la géométrie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "emin, emax = [0.5, 100] * u.TeV\n",
    "\n",
    "energy_axis = MapAxis.from_bounds(\n",
    "    emin.value, emax.value, 20, unit=\"TeV\", name=\"energy\", interp=\"log\"\n",
    ")\n",
    "geom = WcsGeom.create(\n",
    "    skydir=(0, 0),\n",
    "    binsz=0.02,\n",
    "    width=(10, 8),\n",
    "    coordsys=\"GAL\",\n",
    "    proj=\"CAR\",\n",
    "    axes=[energy_axis],\n",
    ")\n",
    "\n",
    "energy_axis_true = MapAxis.from_bounds(\n",
    "    0.1, 200, 20, unit=\"TeV\", name=\"energy\", interp=\"log\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapdataset_dict = {}\n",
    "\n",
    "for k in range (2004,2020):\n",
    "    name = \"map\" + str(k)\n",
    "    mapdataset_dict[k] = MapDataset.create(\n",
    "    geom=geom, energy_axis_true=energy_axis_true, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fabrication des mapdatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: VerifyWarning: Invalid keyword for column 8: Column null option (TNULLn) is invalid for binary table columns of type '1E' (got -1).  The invalid value will be ignored for the purpose of formatting the data in this column. [astropy.io.fits.column]\n",
      "WARNING: VerifyWarning: Invalid keyword for column 21: Column null option (TNULLn) is invalid for binary table columns of type '1E' (got -1).  The invalid value will be ignored for the purpose of formatting the data in this column. [astropy.io.fits.column]\n",
      "WARNING: VerifyWarning: Invalid keyword for column 23: Column null option (TNULLn) is invalid for binary table columns of type '1E' (got -1).  The invalid value will be ignored for the purpose of formatting the data in this column. [astropy.io.fits.column]\n",
      "WARNING: VerifyWarning: Invalid keyword for column 25: Column null option (TNULLn) is invalid for binary table columns of type '1E' (got -1).  The invalid value will be ignored for the purpose of formatting the data in this column. [astropy.io.fits.column]\n",
      "WARNING: VerifyWarning: Invalid keyword for column 26: Column null option (TNULLn) is invalid for binary table columns of type '1D' (got -1).  The invalid value will be ignored for the purpose of formatting the data in this column. [astropy.io.fits.column]\n",
      "WARNING: VerifyWarning: Invalid keyword for column 27: Column null option (TNULLn) is invalid for binary table columns of type '1E' (got -1).  The invalid value will be ignored for the purpose of formatting the data in this column. [astropy.io.fits.column]\n",
      "WARNING: VerifyWarning: Invalid keyword for column 28: Column null option (TNULLn) is invalid for binary table columns of type '1E' (got -1).  The invalid value will be ignored for the purpose of formatting the data in this column. [astropy.io.fits.column]\n",
      "WARNING: VerifyWarning: Invalid keyword for column 29: Column null option (TNULLn) is invalid for binary table columns of type '1E' (got -1).  The invalid value will be ignored for the purpose of formatting the data in this column. [astropy.io.fits.column]\n",
      "WARNING: VerifyWarning: Invalid keyword for column 30: Column null option (TNULLn) is invalid for binary table columns of type '1E' (got -1).  The invalid value will be ignored for the purpose of formatting the data in this column. [astropy.io.fits.column]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 56s, sys: 1.87 s, total: 15min 58s\n",
      "Wall time: 15min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "exclusion_region = RectangleSkyRegion(src_pos, 3*u.deg, 1*u.deg)\n",
    "exclusion_mask = geom.region_mask([exclusion_region], inside=False)\n",
    "exclusion_mask = Map.from_geom(geom, data=exclusion_mask)\n",
    "\n",
    "for year in range(2004,2020):\n",
    "    \n",
    "    offset_max = 2.0 * u.deg\n",
    "    maker = MapDatasetMaker()\n",
    "    maker_safe_mask = SafeMaskMaker(methods=[\"offset-max\", \"bkg-peak\"], offset_max=offset_max)\n",
    "    maker_bkg = FoVBackgroundMaker(\"scale\", exclusion_mask)\n",
    "    \n",
    "    # au lieu de \"scale\" prendre \"fit\" en donnant au dataset un masque sur les bin en énergie (prendre à partir de 1 TeV par ex, même si c'est déjà haut 500 GeV)\n",
    "    # ou alors on met \"bkg-peak\" pour le safemaskmaker (le premier bin en énergie est retiré)\n",
    "    \n",
    "    # refaire des mapdataset en excluant le premier bin pour voir si ça change\n",
    "    \n",
    "    spectrum = PowerLaw2SpectralModel(index=2.3)\n",
    "\n",
    "    for obs in yearly_obs[year]:\n",
    "        # First a cutout of the target map is produced\n",
    "        cutout = mapdataset_dict[year].cutout(obs.pointing_radec, width=2 * offset_max)\n",
    "\n",
    "        # A MapDataset is filled in this cutout geometry\n",
    "        dataset = maker.run(cutout, obs)\n",
    "        \n",
    "        # The data quality cut is applied\n",
    "        dataset = maker_safe_mask.run(dataset, obs)\n",
    "        \n",
    "        dataset = maker_bkg.run(dataset)\n",
    "        \n",
    "        # The resulting dataset cutout is stacked onto the final one\n",
    "        mapdataset_dict[year].stack(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "safemask = mapdataset_dict[2004].mask_safe\n",
    "expo = mapdataset_dict[2004].exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce = safemask.reduce_over_axes(func=np.logical_or)\n",
    "safemask.data[:, 250 ,250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lors du stack on n'est pas surs de comment les différents seuils sont pris en compte.\n",
    "Regardez les différentes énergies seuils, voir la distribution et enlever ceux qui sont trop haut. Ou bien juste partir beaucoup plus haut que le seuil donné par bkg-peak.\n",
    "\n",
    "Eventuellement participer à l'implémentation de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour information, la fonction associée à 'bkg-peak', rien à changer a priori, c'est simple et clair. ça fait ce que c'est censé faire, \n",
    "# à la limite on peut penser à d'autres méthodes pour couper en énergie\n",
    "\n",
    "def make_mask_energy_bkg_peak(dataset):\n",
    "        \"\"\"Make safe energy mask based on the binned background.\n",
    "\n",
    "        The energy threshold is defined as the upper edge of the energy\n",
    "        bin with the highest predicted background rate. This method is motivated\n",
    "        by its use in the HESS DL3 validation paper: https://arxiv.org/pdf/1910.08088.pdf\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : `~gammapy.modeling.Dataset`\n",
    "            Dataset to compute mask for.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mask_safe : `~numpy.ndarray`\n",
    "            Safe data range mask.\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(dataset, (MapDataset, MapDatasetOnOff)):\n",
    "            background_spectrum = dataset.background_model.map.get_spectrum()\n",
    "            counts = dataset.counts.geom\n",
    "        else:\n",
    "            background_spectrum = dataset.background\n",
    "            counts = dataset.counts\n",
    "\n",
    "        idx = np.argmax(background_spectrum.data)         # trouve l'indice du max du spectre du bkg\n",
    "        e_min = background_spectrum.energy.edges[idx + 1] # identifie l'emin comme l'énergie suivante du max\n",
    "        return counts.energy_mask(emin=e_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour information, la méthode 'stack' de la classe MapDataset, on a un mapdataset (self) auquel on en rajoute un autre (other)\n",
    "\n",
    "def stack(self, other):\n",
    "        \"\"\"Stack another dataset in place.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        other: `~gammapy.cube.MapDataset`\n",
    "            Map dataset to be stacked with this one.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.counts and other.counts:\n",
    "            self.counts *= self.mask_safe\n",
    "            self.counts.stack(other.counts, weights=other.mask_safe)\n",
    "\n",
    "        if self.exposure and other.exposure:                                     # ICI\n",
    "            mask_image = self.mask_safe.reduce_over_axes(func=np.logical_or)\n",
    "            self.exposure *= mask_image.data\n",
    "            \n",
    "            # TODO: apply energy dependent mask to exposure. Does this require\n",
    "            #  a mask_safe in true energy?\n",
    "            \n",
    "            mask_image_other = other.mask_safe.reduce_over_axes(func=np.logical_or)\n",
    "            self.exposure.stack(other.exposure, weights=mask_image_other)\n",
    "            \n",
    "            # on applique reduce_over_axes aux deux safe_mask, donc on les réduit sur les axes non-spatiaux \n",
    "            # (mais on prend le meilleur masque si les énergies sont masquées différemment)\n",
    "            # donc on ne prend pas en compte le masquage éventuel en énergie lorsqu'on modifie l'exposure, \n",
    "            # dans : self.exposure *= mask_image.data (elle est multipliée d'un bloc)\n",
    "            \n",
    "            # derrière il semble avoir un souci entre énergie vraie et énergie recombinée\n",
    "\n",
    "            \n",
    "        if self.background_model and other.background_model:    \n",
    "            bkg = self.background_model.evaluate()\n",
    "            bkg *= self.mask_safe\n",
    "            other_bkg = other.background_model.evaluate()\n",
    "            bkg.stack(other_bkg, weights=other.mask_safe)\n",
    "\n",
    "            self.background_model = BackgroundModel(\n",
    "                bkg, name=self.background_model.name\n",
    "            )\n",
    "\n",
    "        if self.mask_safe is not None and other.mask_safe is not None:            # ICI en fait non ya rien à changer ici\n",
    "            self.mask_safe.stack(other.mask_safe)\n",
    "            \n",
    "            # juste une fonction 'stack' celle de WcsNDMap\n",
    "\n",
    "        if self.psf and other.psf:\n",
    "            if isinstance(self.psf, PSFMap) and isinstance(other.psf, PSFMap):\n",
    "                mask_irf = self._mask_safe_irf(self.psf.psf_map, mask_image)\n",
    "                self.psf.psf_map *= mask_irf.data\n",
    "                self.psf.exposure_map *= mask_irf.data\n",
    "\n",
    "                mask_image_other = other.mask_safe.reduce_over_axes(func=np.logical_or)\n",
    "                mask_irf_other = self._mask_safe_irf(\n",
    "                    other.psf.psf_map, mask_image_other\n",
    "                )\n",
    "                self.psf.stack(other.psf, weights=mask_irf_other)\n",
    "            else:\n",
    "                raise ValueError(\"Stacking of PSF kernels not supported\")\n",
    "\n",
    "        if self.edisp and other.edisp:\n",
    "            if isinstance(self.edisp, EDispMap) and isinstance(other.edisp, EDispMap):\n",
    "                mask_irf = self._mask_safe_irf(self.edisp.edisp_map, mask_image)\n",
    "                self.edisp.edisp_map *= mask_irf.data\n",
    "                self.edisp.exposure_map *= mask_irf.data\n",
    "\n",
    "                mask_image_other = other.mask_safe.reduce_over_axes(func=np.logical_or)\n",
    "                mask_irf_other = self._mask_safe_irf(\n",
    "                    other.edisp.edisp_map, mask_image_other\n",
    "                )\n",
    "                self.edisp.stack(other.edisp, weights=mask_irf_other)\n",
    "            else:\n",
    "                raise ValueError(\"Stacking of edisp kernels not supported\")\n",
    "\n",
    "        if self.gti and other.gti:\n",
    "            self.gti = self.gti.stack(other.gti).union()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour info, méthode stack de la classe WcsNDMap\n",
    "\n",
    "def stack(self, other, weights=None):\n",
    "        \"\"\"Stack cutout into map.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        other : `WcsNDMap`\n",
    "            Other map to stack\n",
    "        weights : `WcsNDMap`\n",
    "            Array to be used as weights.\n",
    "        \"\"\"\n",
    "        if self.geom == other.geom:\n",
    "            parent_slices, cutout_slices = None, None\n",
    "        elif self.geom.is_aligned(other.geom):\n",
    "            slices = other.geom.cutout_info[\"parent-slices\"]\n",
    "            parent_slices = Ellipsis, slices[0], slices[1]\n",
    "\n",
    "            slices = other.geom.cutout_info[\"cutout-slices\"]\n",
    "            cutout_slices = Ellipsis, slices[0], slices[1]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Can only stack equivalent maps or cutout of the same map.\"\n",
    "            )\n",
    "\n",
    "        data = other.data[cutout_slices]\n",
    "\n",
    "        if weights is not None:\n",
    "            data = data * weights.data\n",
    "\n",
    "        self.data[parent_slices] += data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapdataset_dict[2006].mask_safe.data[:,250,250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.7          0.89710471   1.14970981   1.47344298   1.88833234\n",
      "   2.42004549   3.10147745   3.97478577   5.09399863   6.5283574\n",
      "   8.36660027  10.72245218  13.74166054  17.61101204  22.56988842\n",
      "  28.92507609  37.06974581  47.50777665  60.88492901  78.02879533\n",
      " 100.        ] TeV\n"
     ]
    }
   ],
   "source": [
    "print(energy_axis.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour masquer plus de bins que ce que fait le mask safe\n",
    "\n",
    "mask = dataset.mask_safe.copy()\n",
    "mask.data[0:3,:,:]=False\n",
    "dataset.mask_fit = mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegarde des mapdatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "#path = Path(\"$GAMMAPY_DATA/mapdataset_hess/mapsdataset_bkg-peak\")\n",
    "#path.mkdir(exist_ok=True)\n",
    "\n",
    "for year in range(2004,2020):\n",
    "    \n",
    "    filename = \"$GAMMAPY_DATA/mapdataset_hess/mapsdataset_700GeV_sans_bkg-peak/mapdataset\" +str(year)+\".fits.gz\"\n",
    "    mapdataset_dict[year].write(filename, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
