{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolution de l'émission du centre galactique et de l'émission diffuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.15'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "import astropy.units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.convolution import Tophat2DKernel\n",
    "from regions import CircleSkyRegion, RectangleSkyRegion\n",
    "\n",
    "from gammapy.detect import compute_lima_on_off_image\n",
    "from gammapy.data import DataStore\n",
    "from gammapy.irf import make_mean_psf\n",
    "from gammapy.maps import Map, MapAxis, WcsGeom\n",
    "from gammapy.cube import (\n",
    "    MapDatasetMaker,\n",
    "    PSFKernel,\n",
    "    MapDataset,\n",
    "    RingBackgroundMaker,\n",
    "    SafeMaskMaker,\n",
    "    #RingBackgroundEstimator,\n",
    ")\n",
    "from gammapy.modeling.models import (\n",
    "    SkyModel,\n",
    "    BackgroundModel,\n",
    "    PowerLawSpectralModel,\n",
    "    PowerLaw2SpectralModel,\n",
    "    PointSpatialModel,\n",
    "    ExpCutoffPowerLawSpectralModel,\n",
    "    SkyDiffuseCube,\n",
    "    TemplateSpatialModel\n",
    ")\n",
    "from gammapy.modeling import Fit\n",
    "from astropy.time import Time\n",
    "\n",
    "src_pos = SkyCoord(359.94, -0.04, unit=\"deg\", frame=\"galactic\")\n",
    "\n",
    "import gammapy\n",
    "gammapy.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emprunt d'une classe implémentée en 0.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##FoV background estimation\n",
    "import logging\n",
    "from gammapy.maps import Map\n",
    "from gammapy.modeling import Fit, Datasets\n",
    "\n",
    "\n",
    "class FoVBackgroundMaker:\n",
    "    \"\"\"Normalize template background on the whole field-of-view.\n",
    "\n",
    "    The dataset background model can be simply scaled (method=\"scale\") or fitted (method=\"fit\")\n",
    "    on the dataset counts.\n",
    "\n",
    "    The normalization is performed outside the exclusion mask that is passed on init.\n",
    "\n",
    "    If a SkyModel is set on the input dataset and method is 'fit', its are frozen during\n",
    "    the fov normalization fit.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    method : str in ['fit', 'scale']\n",
    "        the normalization method to be applied. Default 'scale'.\n",
    "    exclusion_mask : `~gammapy.maps.WcsNDMap`\n",
    "        Exclusion mask\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, method=\"scale\", exclusion_mask=None):\n",
    "        if method in [\"fit\", \"scale\"]:\n",
    "            self.method = method\n",
    "        else:\n",
    "            raise ValueError(f\"Incorrect method for FoVBackgroundMaker: {method}.\")\n",
    "        self.exclusion_mask = exclusion_mask\n",
    "\n",
    "\n",
    "    def run(self, dataset):\n",
    "        \"\"\"Run FoV background maker.\n",
    "\n",
    "        Fit the background model norm\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : `~gammapy.cube.fit.MapDataset`\n",
    "            Input map dataset.\n",
    "\n",
    "        \"\"\"\n",
    "        mask_fit = dataset.mask_fit\n",
    "        dataset.mask_fit = self._reproject_exclusion_mask(dataset)\n",
    "\n",
    "        if self.method is \"fit\":\n",
    "            self._fit_bkg(dataset)\n",
    "        else:\n",
    "            self._scale_bkg(dataset)\n",
    "\n",
    "        dataset.mask_fit = mask_fit\n",
    "        return dataset\n",
    "\n",
    "\n",
    "    def _reproject_exclusion_mask(self, dataset):\n",
    "        \"\"\"Reproject the exclusion on the dataset geometry\"\"\"\n",
    "        mask_map = Map.from_geom(dataset.counts.geom)\n",
    "        if self.exclusion_mask is not None:\n",
    "            coords = dataset.counts.geom.get_coord()\n",
    "            vals = self.exclusion_mask.get_by_coord(coords)\n",
    "            mask_map.data += vals\n",
    "\n",
    "        return mask_map.data.astype(\"bool\")\n",
    "\n",
    "    def _fit_bkg(self, dataset):\n",
    "        \"\"\"Fit the FoV background model on the dataset counts data\"\"\"\n",
    "\n",
    "        # freeze all model components not related to background model\n",
    "        datasets = Datasets([dataset])\n",
    "\n",
    "        parameters_frozen = []\n",
    "        for par in datasets.parameters:\n",
    "            parameters_frozen.append(par.frozen)\n",
    "            if par not in dataset.background_model.parameters:\n",
    "                par.frozen = True\n",
    "\n",
    "        #!!!AL: relax titlt : BE CARREFULL !!!\n",
    "        dataset.background_model.tilt.frozen=False\n",
    "        \n",
    "        fit = Fit(datasets)\n",
    "        fit_result = fit.run()\n",
    "        if fit_result.success is False:\n",
    "            print(\"FoVBackgroundMaker failed. No fit convergence\")\n",
    "            \n",
    "\n",
    "        # Unfreeze parameters\n",
    "        for i, par in enumerate(datasets.parameters):\n",
    "            par.frozen = parameters_frozen[i]\n",
    "\n",
    "    def _scale_bkg(self, dataset):\n",
    "        \"\"\"Fit the FoV background model on the dataset counts data\"\"\"\n",
    "        mask = dataset.mask\n",
    "        count_tot = dataset.counts.data[mask].sum()\n",
    "        bkg_tot = dataset.background_model.map.data[mask].sum()\n",
    "\n",
    "        if count_tot <= 0.0:\n",
    "            print(\"FoVBackgroundMaker failed. No counts found outside exclusion mask\")\n",
    "        elif bkg_tot <= 0.0:\n",
    "            print(\"FoVBackgroundMaker failed. No positive background found outside exclusion mask\")\n",
    "        else:\n",
    "            scale = count_tot / bkg_tot\n",
    "            dataset.background_model.norm.value = scale\n",
    "            #print(\"bkg scale = \",scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fabrication des mapdatasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define which data to use and print some information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data store:\n",
      "HDU index table:\n",
      "BASE_DIR: /home/samuel/code/gammapy_data/ash_stereo_Prod17_Calib0834_thsq64\n",
      "Rows: 122853\n",
      "OBS_ID: 18092 -- 151486\n",
      "HDU_TYPE: ['aeff', 'bkg', 'edisp', 'events', 'gti', 'psf']\n",
      "HDU_CLASS: ['aeff_2d', 'bkg_2d', 'edisp_2d', 'events', 'gti', 'psf_3gauss', 'psf_table']\n",
      "\n",
      "\n",
      "Observation table:\n",
      "Observatory name: 'N/A'\n",
      "Number of observations: 20485\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_store = DataStore.from_dir(\"$GAMMAPY_DATA/ash_stereo_Prod17_Calib0834_thsq64\")\n",
    "data_store.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.coordinates import Angle\n",
    "\n",
    "\n",
    "selection = dict(type='sky_circle', frame='galactic',\n",
    "                 lon=Angle(0, 'deg'),\n",
    "                 lat=Angle(0, 'deg'),\n",
    "                 radius=Angle(2, 'deg'),\n",
    "                 border=Angle(0, 'deg'))\n",
    "\n",
    "obs_table = data_store.obs_table.select_observations(selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2004  = dict(type='time_box', time_range= Time(['2004-01-01T00:00:00', '2004-12-31T23:59:59']))\n",
    "t2005  = dict(type='time_box', time_range= Time(['2005-01-01T00:00:00', '2005-12-31T23:59:59']))\n",
    "t2006  = dict(type='time_box', time_range= Time(['2006-01-01T00:00:00', '2006-12-31T23:59:59']))\n",
    "t2007  = dict(type='time_box', time_range= Time(['2007-01-01T00:00:00', '2007-12-31T23:59:59']))\n",
    "t2008  = dict(type='time_box', time_range= Time(['2008-01-01T00:00:00', '2008-12-31T23:59:59']))\n",
    "t2009  = dict(type='time_box', time_range= Time(['2009-01-01T00:00:00', '2009-12-31T23:59:59']))\n",
    "t2010  = dict(type='time_box', time_range= Time(['2010-01-01T00:00:00', '2010-12-31T23:59:59']))\n",
    "t2011  = dict(type='time_box', time_range= Time(['2011-01-01T00:00:00', '2011-12-31T23:59:59']))\n",
    "t2012  = dict(type='time_box', time_range= Time(['2012-01-01T00:00:00', '2012-12-31T23:59:59']))\n",
    "t2013  = dict(type='time_box', time_range= Time(['2013-01-01T00:00:00', '2013-12-31T23:59:59']))\n",
    "t2014  = dict(type='time_box', time_range= Time(['2014-01-01T00:00:00', '2014-12-31T23:59:59']))\n",
    "t2015  = dict(type='time_box', time_range= Time(['2015-01-01T00:00:00', '2015-12-31T23:59:59']))\n",
    "t2016  = dict(type='time_box', time_range= Time(['2016-01-01T00:00:00', '2016-12-31T23:59:59']))\n",
    "t2017  = dict(type='time_box', time_range= Time(['2017-01-01T00:00:00', '2017-12-31T23:59:59']))\n",
    "t2018  = dict(type='time_box', time_range= Time(['2018-01-01T00:00:00', '2018-12-31T23:59:59']))\n",
    "t2019  = dict(type='time_box', time_range= Time(['2019-01-01T00:00:00', '2019-12-31T23:59:59']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sélection par année et tri des observations (on retire celles qui n'ont pas toutes les IRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found multiple HDU matching: OBS_ID = 20191, HDU_TYPE = psf, HDU_CLASS = None. Returning the first entry, which has HDU_TYPE = psf and HDU_CLASS = psf_3gauss\n",
      "Found multiple HDU matching: OBS_ID = 20193, HDU_TYPE = psf, HDU_CLASS = None. Returning the first entry, which has HDU_TYPE = psf and HDU_CLASS = psf_3gauss\n",
      "Found multiple HDU matching: OBS_ID = 20194, HDU_TYPE = psf, HDU_CLASS = None. Returning the first entry, which has HDU_TYPE = psf and HDU_CLASS = psf_3gauss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation retirée : 20191\n",
      "Observation retirée : 20193\n",
      "Observation retirée : 20194\n",
      "Observation retirée : 31539\n",
      "Observation retirée : 31577\n",
      "Observation retirée : 31578\n",
      "Observation retirée : 31579\n",
      "Observation retirée : 31580\n"
     ]
    }
   ],
   "source": [
    "year_intervals = { 2004 : t2004, 2005 : t2005, 2006 : t2006, 2007 : t2007,\n",
    "                      2008 : t2008, 2009 : t2009, 2010 : t2010, 2011 : t2011,\n",
    "                      2012 : t2012, 2013 : t2013, 2014 : t2014, 2015 : t2015,\n",
    "                      2016 : t2016, 2017 : t2017, 2018 : t2018, 2019 : t2019}\n",
    "\n",
    "yearly_obs = dict()\n",
    "\n",
    "for year in range(2004,2020) :\n",
    "    \n",
    "    obs_table_year = obs_table.select_observations(year_intervals[year])\n",
    "    ids = obs_table_year[\"OBS_ID\"].tolist()\n",
    "    observations_year = data_store.get_observations(ids, skip_missing=True)\n",
    "    \n",
    "    for obs in observations_year:\n",
    "        try:\n",
    "            obs.aeff\n",
    "            obs.edisp\n",
    "            obs.psf\n",
    "        except:\n",
    "            ids.remove(obs.obs_id)\n",
    "            print(\"Observation retirée : \" + str(obs.obs_id))\n",
    "            \n",
    "    observations_year = data_store.get_observations(ids, skip_missing=True)\n",
    "    yearly_obs[year] = observations_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création de la géométrie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "emin, emax = [0.5, 100] * u.TeV\n",
    "\n",
    "energy_axis = MapAxis.from_bounds(\n",
    "    emin.value, emax.value, 20, unit=\"TeV\", name=\"energy\", interp=\"log\"\n",
    ")\n",
    "geom = WcsGeom.create(\n",
    "    skydir=(0, 0),\n",
    "    binsz=0.02,\n",
    "    width=(10, 8),\n",
    "    coordsys=\"GAL\",\n",
    "    proj=\"CAR\",\n",
    "    axes=[energy_axis],\n",
    ")\n",
    "\n",
    "energy_axis_true = MapAxis.from_bounds(\n",
    "    0.1, 200, 20, unit=\"TeV\", name=\"energy\", interp=\"log\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapdataset_dict = {}\n",
    "\n",
    "for k in range (2004,2020):\n",
    "    name = \"map\" + str(k)\n",
    "    mapdataset_dict[k] = MapDataset.create(\n",
    "    geom=geom, energy_axis_true=energy_axis_true, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fabrication des mapdatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: VerifyWarning: Invalid keyword for column 8: Column null option (TNULLn) is invalid for binary table columns of type '1E' (got -1).  The invalid value will be ignored for the purpose of formatting the data in this column. [astropy.io.fits.column]\n",
      "WARNING: VerifyWarning: Invalid keyword for column 21: Column null option (TNULLn) is invalid for binary table columns of type '1E' (got -1).  The invalid value will be ignored for the purpose of formatting the data in this column. [astropy.io.fits.column]\n",
      "WARNING: VerifyWarning: Invalid keyword for column 23: Column null option (TNULLn) is invalid for binary table columns of type '1E' (got -1).  The invalid value will be ignored for the purpose of formatting the data in this column. [astropy.io.fits.column]\n",
      "WARNING: VerifyWarning: Invalid keyword for column 25: Column null option (TNULLn) is invalid for binary table columns of type '1E' (got -1).  The invalid value will be ignored for the purpose of formatting the data in this column. [astropy.io.fits.column]\n",
      "WARNING: VerifyWarning: Invalid keyword for column 26: Column null option (TNULLn) is invalid for binary table columns of type '1D' (got -1).  The invalid value will be ignored for the purpose of formatting the data in this column. [astropy.io.fits.column]\n",
      "WARNING: VerifyWarning: Invalid keyword for column 27: Column null option (TNULLn) is invalid for binary table columns of type '1E' (got -1).  The invalid value will be ignored for the purpose of formatting the data in this column. [astropy.io.fits.column]\n",
      "WARNING: VerifyWarning: Invalid keyword for column 28: Column null option (TNULLn) is invalid for binary table columns of type '1E' (got -1).  The invalid value will be ignored for the purpose of formatting the data in this column. [astropy.io.fits.column]\n",
      "WARNING: VerifyWarning: Invalid keyword for column 29: Column null option (TNULLn) is invalid for binary table columns of type '1E' (got -1).  The invalid value will be ignored for the purpose of formatting the data in this column. [astropy.io.fits.column]\n",
      "WARNING: VerifyWarning: Invalid keyword for column 30: Column null option (TNULLn) is invalid for binary table columns of type '1E' (got -1).  The invalid value will be ignored for the purpose of formatting the data in this column. [astropy.io.fits.column]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 38s, sys: 2.92 s, total: 15min 41s\n",
      "Wall time: 15min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "exclusion_region = RectangleSkyRegion(src_pos, 3*u.deg, 1*u.deg)\n",
    "exclusion_mask = geom.region_mask([exclusion_region], inside=False)\n",
    "exclusion_mask = Map.from_geom(geom, data=exclusion_mask)\n",
    "\n",
    "for year in range(2004,2020):\n",
    "    \n",
    "    offset_max = 2.0 * u.deg\n",
    "    maker = MapDatasetMaker()\n",
    "    maker_safe_mask = SafeMaskMaker(methods=[\"offset-max\", \"bkg-peak\"], offset_max=offset_max)\n",
    "    maker_bkg = FoVBackgroundMaker(\"scale\", exclusion_mask)\n",
    "    \n",
    "    # au lieu de \"scale\" prendre \"fit\" en donnant au dataset un masque sur les bin en énergie (prendre à partir de 1 TeV par ex, même si c'est déjà haut 500 GeV)\n",
    "    # ou alors on met \"bkg-peak\" pour le safemaskmaker\n",
    "    \n",
    "    spectrum = PowerLaw2SpectralModel(index=2.3)\n",
    "\n",
    "    for obs in yearly_obs[year]:\n",
    "        # First a cutout of the target map is produced\n",
    "        cutout = mapdataset_dict[year].cutout(obs.pointing_radec, width=2 * offset_max)\n",
    "\n",
    "        # A MapDataset is filled in this cutout geometry\n",
    "        dataset = maker.run(cutout, obs)\n",
    "        \n",
    "        # The data quality cut is applied\n",
    "        dataset = maker_safe_mask.run(dataset, obs)\n",
    "        \n",
    "        dataset = maker_bkg.run(dataset)\n",
    "        \n",
    "        # The resulting dataset cutout is stacked onto the final one\n",
    "        mapdataset_dict[year].stack(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauvegarde des mapdatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "#path = Path(\"$GAMMAPY_DATA/mapdataset_hess/mapsdataset_bkg-peak\")\n",
    "#path.mkdir(exist_ok=True)\n",
    "\n",
    "for year in range(2004,2020):\n",
    "    \n",
    "    filename = \"$GAMMAPY_DATA/mapdataset_hess/mapsdataset_bkg-peak/mapdataset\" +str(year)+\".fits.gz\"\n",
    "    mapdataset_dict[year].write(filename, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
